{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Lógica de NN para el 2 armed bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# ====== 1. BANDIT ENVIRONMENT ======\n",
    "\n",
    "class TwoArmedBandit:\n",
    "    def __init__(self):\n",
    "        # En cada episodio cambiamos las probabilidades\n",
    "        p = np.random.uniform(0.1, 0.9)\n",
    "        self.probs = [p, 1 - p]\n",
    "\n",
    "    def pull(self, action):\n",
    "        return 1 if np.random.rand() < self.probs[action] else 0\n",
    "\n",
    "\n",
    "# ====== 2. META-RL AGENT (LSTM) ======\n",
    "\n",
    "class MetaRLAgent(nn.Module):\n",
    "    def __init__(self, input_size=3, hidden_size=32):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTMCell(input_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, 2)  # 2 acciones\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.reset_state()\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.hx = torch.zeros(1, self.hidden_size)\n",
    "        self.cx = torch.zeros(1, self.hidden_size)\n",
    "\n",
    "    def forward(self, action, reward, timestep):\n",
    "        # Entrada: [última acción (one-hot), última recompensa, timestep normalizado]\n",
    "        action_one_hot = F.one_hot(torch.tensor([action]), num_classes=2).float()\n",
    "        reward = torch.tensor([[reward]], dtype=torch.float32)\n",
    "        timestep = torch.tensor([[timestep / 10.0]])  # normalizar\n",
    "\n",
    "        x = torch.cat([action_one_hot, reward, timestep], dim=1)\n",
    "        self.hx, self.cx = self.lstm(x, (self.hx, self.cx))\n",
    "        logits = self.fc(self.hx)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Ejecutar el agente entrenado (aún falta implementar el entrenamiento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Episodio de ejemplo con pesos fijos (simulación de post-entrenamiento)\n",
    "agent = MetaRLAgent()\n",
    "agent.eval()\n",
    "\n",
    "env = TwoArmedBandit()\n",
    "agent.reset_state()\n",
    "\n",
    "last_action = 0\n",
    "last_reward = 0\n",
    "total_reward = 0\n",
    "\n",
    "print(\"Probabilidades ocultas del entorno:\", env.probs)\n",
    "\n",
    "for t in range(5):  # 5 pasos\n",
    "    with torch.no_grad():\n",
    "        logits = agent(last_action, last_reward, t)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        action = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "    reward = env.pull(action)\n",
    "    total_reward += reward\n",
    "\n",
    "    print(f\"Paso {t} | Acción: {action} | Recompensa: {reward}\")\n",
    "\n",
    "    last_action = action\n",
    "    last_reward = reward\n",
    "\n",
    "print(\"Recompensa total:\", total_reward)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
