{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Lógica de NN para el 2 armed bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# ======= 1. ENTORNO: Bandit de 2 brazos =======\n",
    "class TwoArmedBandit:\n",
    "    def __init__(self):\n",
    "        # Cada episodio, la probabilidad para el brazo 0 se elige aleatoriamente en [0.1, 0.9].\n",
    "        # El brazo 1 tendrá probabilidad = 1 - p\n",
    "        p = np.random.uniform(0.1, 0.9)\n",
    "        self.probs = [p, 1 - p]\n",
    "    \n",
    "    def pull(self, action):\n",
    "        # Regresa 1 con la probabilidad correspondiente o 0\n",
    "        return 1 if np.random.rand() < self.probs[action] else 0\n",
    "\n",
    "# ======= 2. MODELO: Agente PPO con LSTM =======\n",
    "class PPOAgent(nn.Module):\n",
    "    def __init__(self, input_size=4, hidden_size=32, num_actions=2):\n",
    "        \"\"\"\n",
    "        input_size: dimensión del vector de entrada. Aquí se usa one-hot para la acción (2),\n",
    "                    la recompensa previa (1) y el timestep (1) => 2+1+1=4.\n",
    "        \"\"\"\n",
    "        super(PPOAgent, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTMCell(input_size, hidden_size)\n",
    "        self.policy_head = nn.Linear(hidden_size, num_actions)  # salida de logits para 2 acciones\n",
    "        self.value_head = nn.Linear(hidden_size, 1)             # salida de valor\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.hx = torch.zeros(1, self.hidden_size)\n",
    "        self.cx = torch.zeros(1, self.hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x es de tamaño (1, input_size)\n",
    "        self.hx, self.cx = self.lstm(x, (self.hx, self.cx))\n",
    "        logits = self.policy_head(self.hx)\n",
    "        value = self.value_head(self.hx)\n",
    "        return logits, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Entrenamiento con PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= 3. Crear la entrada del agente =======\n",
    "# Formato de LSTM \n",
    "def get_input(last_action, last_reward, timestep, num_actions=2):\n",
    "    # Se construye un vector que contiene:\n",
    "    # - la acción previa en formato one-hot (dim=2)\n",
    "    # - la recompensa previa (dim=1)\n",
    "    # - el timestep normalizado (dim=1) (dividido por 10, para mantener magnitudes similares)\n",
    "    action_one_hot = F.one_hot(torch.tensor([last_action]), num_classes=num_actions).float()\n",
    "    reward_tensor = torch.tensor([[last_reward]], dtype=torch.float32)\n",
    "    timestep_tensor = torch.tensor([[timestep / 10.0]], dtype=torch.float32)\n",
    "    x = torch.cat([action_one_hot, reward_tensor, timestep_tensor], dim=1)\n",
    "    return x\n",
    "\n",
    "# ======= 4. HIPERPARÁMETROS DE PPO =======\n",
    "gamma = 0.99           # factor de descuento\n",
    "clip_epsilon = 0.2     # parámetro de recorte PPO\n",
    "ppo_epochs = 4         # número de épocas por actualización\n",
    "lr = 0.009             # tasa de aprendizaje (número mágico que aprendí en la concentración)\n",
    "\n",
    "agent = PPOAgent()\n",
    "optimizer = optim.Adam(agent.parameters(), lr=lr)\n",
    "\n",
    "num_episodes = 1000    # cantidad total de episodios\n",
    "episode_length = 5     # pasos por episodio\n",
    "\n",
    "# ======= 5. CICLO DE ENTRENAMIENTO CON PPO =======\n",
    "for episode in range(num_episodes):\n",
    "    env = TwoArmedBandit()\n",
    "    agent.reset_state()\n",
    "    \n",
    "    # Listas para almacenar la trayectoria\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    log_probs = []\n",
    "    values = []\n",
    "    \n",
    "    # Inicializamos con una acción por defecto (0) y recompensa 0 para el primer paso\n",
    "    last_action = 0\n",
    "    last_reward = 0.0\n",
    "    \n",
    "    # Recorrido del episodio\n",
    "    for t in range(episode_length):\n",
    "        x = get_input(last_action, last_reward, t)\n",
    "        logits, value = agent(x)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        \n",
    "        # Almacenamos los datos para esta transición\n",
    "        states.append(x)\n",
    "        actions.append(action)\n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value)\n",
    "        \n",
    "        # Tomamos la acción en el entorno y obtenemos la recompensa\n",
    "        reward = env.pull(action.item())\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        last_action = action.item()\n",
    "        last_reward = reward\n",
    "    \n",
    "    # ======= 5.1. Calcular los RETURNS y las VENTAJAS =======\n",
    "    returns = []\n",
    "    G = 0\n",
    "    for r in reversed(rewards):\n",
    "        G = r + gamma * G\n",
    "        returns.insert(0, G)\n",
    "    returns = torch.tensor(returns, dtype=torch.float32).unsqueeze(1)  # dimensión (episode_length, 1)\n",
    "    values = torch.cat(values)  # dimensión (episode_length, 1)\n",
    "    advantages = returns - values.detach()\n",
    "    \n",
    "    old_log_probs = torch.cat(log_probs).detach()\n",
    "    \n",
    "    # ======= 5.2. Actualización PPO sobre la trayectoria recogida =======\n",
    "    for _ in range(ppo_epochs):\n",
    "        new_log_probs = []\n",
    "        new_values = []\n",
    "        agent.reset_state()  # Reiniciamos el estado para evaluar la trayectoria almacenada\n",
    "        \n",
    "        # Se reevalúa cada estado almacenado\n",
    "        for i, x in enumerate(states):\n",
    "            logits, value = agent(x)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            dist = torch.distributions.Categorical(probs)\n",
    "            new_log_probs.append(dist.log_prob(actions[i]))\n",
    "            new_values.append(value)\n",
    "        new_log_probs = torch.cat(new_log_probs)\n",
    "        new_values = torch.cat(new_values)\n",
    "        \n",
    "        ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "        surr1 = ratio * advantages\n",
    "        surr2 = torch.clamp(ratio, 1 - clip_epsilon, 1 + clip_epsilon) * advantages\n",
    "        policy_loss = -torch.min(surr1, surr2).mean()\n",
    "        value_loss = F.mse_loss(new_values, returns)\n",
    "        loss = policy_loss + 0.5 * value_loss  # combinación de pérdidas\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if (episode+1) % 100 == 0:\n",
    "        total_reward = sum(rewards)\n",
    "        print(f\"Episode {episode+1}, Total Reward: {total_reward}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"Entrenamiento completado.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
