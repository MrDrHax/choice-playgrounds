{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Lógica de NN para el 2 armed bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# ======= 1. ENTORNO: DoorPairBandit con 4 puertas =======\n",
    "class DoorPairBandit:\n",
    "    def __init__(self):\n",
    "        # Definimos los parámetros de cada puerta:\n",
    "        #  A: p=0.75, reward= +10  \n",
    "        #  B: p=0.25, reward= +10  \n",
    "        #  C: p=0.75, reward= +3   \n",
    "        #  D: p=0.25, reward= +3   \n",
    "        mapping = {\n",
    "            'A': (0.75, 10),\n",
    "            'B': (0.25, 10),\n",
    "            'C': (0.75,  3),\n",
    "            'D': (0.25,  3),\n",
    "        }\n",
    "        # Las parejas que queremos muestrear:\n",
    "        pairs = [('A','B'), ('C','D'), ('A','D'), ('B','C')]\n",
    "        # Elegimos aleatoriamente una pareja para el episodio\n",
    "        self.left_label, self.right_label = pairs[np.random.randint(len(pairs))]\n",
    "        self.left_p, self.left_r   = mapping[self.left_label]\n",
    "        self.right_p, self.right_r = mapping[self.right_label]\n",
    "\n",
    "    def pull(self, action):\n",
    "        \"\"\"\n",
    "        action == 0 -> puerta izquierda\n",
    "        action == 1 -> puerta derecha\n",
    "        Devolvemos +rew  o -rew según la probabilidad\n",
    "        \"\"\"\n",
    "        if action == 0:\n",
    "            p, r = self.left_p,  self.left_r\n",
    "        else:\n",
    "            p, r = self.right_p, self.right_r\n",
    "\n",
    "        if np.random.rand() < p:\n",
    "            return  r\n",
    "        else:\n",
    "            return -r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Entrenamiento con PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100, Total Reward: 140, Loss: 1698.6903\n",
      "Episode 200, Total Reward: 60, Loss: 216.9727\n",
      "Episode 300, Total Reward: 42, Loss: 121.1637\n",
      "Episode 400, Total Reward: 42, Loss: 114.0210\n",
      "Episode 500, Total Reward: 36, Loss: 136.1891\n",
      "Episode 600, Total Reward: 60, Loss: 271.1169\n",
      "Episode 700, Total Reward: 24, Loss: 267.9634\n",
      "Episode 800, Total Reward: 120, Loss: 888.8828\n",
      "Episode 900, Total Reward: 100, Loss: 539.2075\n",
      "Episode 1000, Total Reward: 140, Loss: 1436.2120\n",
      "Entrenamiento completado.\n"
     ]
    }
   ],
   "source": [
    "# ======= 2. MODELO: Agente PPO con LSTM =======\n",
    "class PPOAgent(nn.Module):\n",
    "    def __init__(self, input_size=4, hidden_size=32, num_actions=2):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTMCell(input_size, hidden_size)\n",
    "        self.policy_head = nn.Linear(hidden_size, num_actions)\n",
    "        self.value_head  = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.hx = torch.zeros(1, self.hidden_size)\n",
    "        self.cx = torch.zeros(1, self.hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.hx, self.cx = self.lstm(x, (self.hx, self.cx))\n",
    "        return self.policy_head(self.hx), self.value_head(self.hx)\n",
    "\n",
    "# ======= 3. Función para crear la entrada =======\n",
    "def get_input(last_action, last_reward, timestep):\n",
    "    # one-hot acción previa, recompensa previa, timestep normalizado\n",
    "    a = F.one_hot(torch.tensor([last_action]), num_classes=2).float()\n",
    "    r = torch.tensor([[last_reward]], dtype=torch.float32)\n",
    "    t = torch.tensor([[timestep / 10.0]], dtype=torch.float32)\n",
    "    return torch.cat([a, r, t], dim=1)\n",
    "\n",
    "# ======= 4. Hiperparámetros PPO =======\n",
    "gamma        = 0.99\n",
    "clip_epsilon = 0.2\n",
    "ppo_epochs   = 4\n",
    "lr           = 0.001\n",
    "\n",
    "agent     = PPOAgent()\n",
    "optimizer = optim.Adam(agent.parameters(), lr=lr)\n",
    "\n",
    "num_episodes   = 2000\n",
    "episode_length = 10\n",
    "\n",
    "# ======= 5. Entrenamiento PPO =======\n",
    "for episode in range(num_episodes):\n",
    "    env = DoorPairBandit()\n",
    "    agent.reset_state()\n",
    "\n",
    "    states, actions, rewards, logps, values = [], [], [], [], []\n",
    "    last_action, last_reward = 0, 0.0\n",
    "\n",
    "    # Recolectar un episodio\n",
    "    for t in range(episode_length):\n",
    "        x       = get_input(last_action, last_reward, t)\n",
    "        logits, v = agent(x)\n",
    "        probs   = F.softmax(logits, dim=1)\n",
    "        dist    = torch.distributions.Categorical(probs)\n",
    "        action  = dist.sample()\n",
    "        logp    = dist.log_prob(action)\n",
    "\n",
    "        states.append(x)\n",
    "        actions.append(action)\n",
    "        logps.append(logp)\n",
    "        values.append(v)\n",
    "\n",
    "        reward = env.pull(action.item())\n",
    "        rewards.append(reward)\n",
    "\n",
    "        last_action, last_reward = action.item(), reward\n",
    "\n",
    "    # Calcular returns y ventajas\n",
    "    returns, G = [], 0\n",
    "    for r in reversed(rewards):\n",
    "        G = r + gamma * G\n",
    "        returns.insert(0, G)\n",
    "    returns    = torch.tensor(returns, dtype=torch.float32).unsqueeze(1)\n",
    "    values     = torch.cat(values)\n",
    "    advantages = returns - values.detach()\n",
    "    old_logps  = torch.cat(logps).detach()\n",
    "\n",
    "    # Actualización PPO\n",
    "    for _ in range(ppo_epochs):\n",
    "        new_logps, new_vals, entropies = [], [], []\n",
    "        agent.reset_state()\n",
    "        for i, x in enumerate(states):\n",
    "            logits, v = agent(x)\n",
    "            probs  = F.softmax(logits, dim=1)\n",
    "            dist   = torch.distributions.Categorical(probs)\n",
    "            new_logps.append(dist.log_prob(actions[i]))\n",
    "            new_vals.append(v)\n",
    "            entropies.append(dist.entropy())\n",
    "        new_logps     = torch.cat(new_logps)\n",
    "        new_vals      = torch.cat(new_vals)\n",
    "        entropy_term  = torch.cat(entropies).mean()\n",
    "\n",
    "        ratio       = torch.exp(new_logps - old_logps)\n",
    "        s1          = ratio * advantages\n",
    "        s2          = torch.clamp(ratio, 1-clip_epsilon, 1+clip_epsilon) * advantages\n",
    "        policy_loss = -torch.min(s1, s2).mean()\n",
    "        value_loss  = F.mse_loss(new_vals, returns)\n",
    "        loss        = policy_loss + 0.5*value_loss - 0.01*entropy_term\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (episode+1) % 200 == 0:\n",
    "        print(f\"Ep {episode+1}, Reward sum: {sum(rewards)}, Loss: {loss.item():.2f}\")\n",
    "\n",
    "print(\"Entrenamiento completado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Ejemplo de funcionamiento con un agente en modo evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Familia y probs ocultas: [0.75, 0.25] [3, -3]\n",
      "Paso 0 | Acción: 1 | Recompensa: 3\n",
      "Paso 1 | Acción: 1 | Recompensa: 3\n",
      "Paso 2 | Acción: 1 | Recompensa: 3\n",
      "Paso 3 | Acción: 1 | Recompensa: 3\n",
      "Paso 4 | Acción: 1 | Recompensa: 3\n",
      "Paso 5 | Acción: 1 | Recompensa: -3\n",
      "Paso 6 | Acción: 1 | Recompensa: 3\n",
      "Paso 7 | Acción: 1 | Recompensa: 3\n",
      "Paso 8 | Acción: 1 | Recompensa: 3\n",
      "Paso 9 | Acción: 1 | Recompensa: 3\n",
      "Paso 10 | Acción: 1 | Recompensa: -3\n",
      "Paso 11 | Acción: 1 | Recompensa: 3\n",
      "Paso 12 | Acción: 1 | Recompensa: 3\n",
      "Paso 13 | Acción: 1 | Recompensa: 3\n",
      "Paso 14 | Acción: 1 | Recompensa: 3\n",
      "Paso 15 | Acción: 1 | Recompensa: 3\n",
      "Paso 16 | Acción: 1 | Recompensa: -3\n",
      "Paso 17 | Acción: 1 | Recompensa: 3\n",
      "Paso 18 | Acción: 1 | Recompensa: -3\n",
      "Paso 19 | Acción: 1 | Recompensa: 3\n",
      "Recompensa total: 36\n"
     ]
    }
   ],
   "source": [
    "# ======= 6. Evaluación =======\n",
    "agent.eval()\n",
    "env = DoorPairBandit()\n",
    "agent.reset_state()\n",
    "\n",
    "print(\"Puertas:\", env.left_label, \"vs\", env.right_label,\n",
    "      \"| probs:\", env.probs, \"| rews:\", env.rews)\n",
    "\n",
    "last_action, last_reward = 0, 0\n",
    "total_reward = 0\n",
    "\n",
    "for t in range(episode_length):\n",
    "    with torch.no_grad():\n",
    "        x      = get_input(last_action, last_reward, t)\n",
    "        logits, _ = agent(x)\n",
    "        probs  = F.softmax(logits, dim=1)[0].tolist()\n",
    "        action = torch.multinomial(torch.tensor(probs), 1).item()\n",
    "\n",
    "    reward = env.pull(action)\n",
    "    total_reward += reward\n",
    "    print(f\"Paso {t} | probs {probs} | Acción {action} | Recompensa {reward}\")\n",
    "    last_action, last_reward = action, reward\n",
    "\n",
    "print(\"Recompensa total:\", total_reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
