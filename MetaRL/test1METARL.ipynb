{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Lógica de NN para el 2 armed bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# ======= 1. ENTORNO: RestlessBandit de 2 brazos =======\n",
    "class RestlessBandit:\n",
    "    def __init__(self, volatility=0.1):\n",
    "        # Inicializamos p para el Brazo 0 entre 0.1 y 0.9.\n",
    "        # El brazo 1 tendrá probabilidad = 1 - p.\n",
    "        p = np.random.uniform(0.1, 0.9)\n",
    "        self.probs = [p, 1 - p]\n",
    "        self.volatility = volatility\n",
    "\n",
    "    def pull(self, action):\n",
    "        # Cada vez que se llama a pull, se actualizan las probabilidades:\n",
    "        noise = np.random.randn() * self.volatility\n",
    "        new_p = self.probs[0] + noise\n",
    "        # Limitamos p entre 0.1 y 0.9 para evitar extremos.\n",
    "        new_p = np.clip(new_p, 0.1, 0.9)\n",
    "        self.probs[0] = new_p\n",
    "        self.probs[1] = 1 - new_p\n",
    "\n",
    "        # Retorna 1 con probabilidad de self.probs[action] o 0.\n",
    "        return 1 if np.random.rand() < self.probs[action] else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Entrenamiento con PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100, Total Reward: 4, Loss: -0.0631\n",
      "Episode 200, Total Reward: 4, Loss: -0.1427\n",
      "Episode 300, Total Reward: 2, Loss: 0.4094\n",
      "Episode 400, Total Reward: 1, Loss: 1.9543\n",
      "Episode 500, Total Reward: 4, Loss: -0.3483\n",
      "Episode 600, Total Reward: 4, Loss: 0.1085\n",
      "Episode 700, Total Reward: 2, Loss: -0.0126\n",
      "Episode 800, Total Reward: 3, Loss: -0.1049\n",
      "Episode 900, Total Reward: 3, Loss: 1.0354\n",
      "Episode 1000, Total Reward: 3, Loss: -0.3200\n",
      "Entrenamiento completado.\n"
     ]
    }
   ],
   "source": [
    "# ======= 2. MODELO: Agente PPO con LSTM =======\n",
    "class PPOAgent(nn.Module):\n",
    "    def __init__(self, input_size=4, hidden_size=32, num_actions=2):\n",
    "        \"\"\"\n",
    "        input_size: dimensión del vector de entrada. Se compone de:\n",
    "                    - Acción previa en formato one-hot (2 dimensiones)\n",
    "                    - Recompensa previa (1 dimensión)\n",
    "                    - Timestep normalizado (1 dimensión)\n",
    "                    Total: 2+1+1 = 4.\n",
    "        \"\"\"\n",
    "        super(PPOAgent, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTMCell(input_size, hidden_size)\n",
    "        self.policy_head = nn.Linear(hidden_size, num_actions)  # Produce los logits\n",
    "        self.value_head = nn.Linear(hidden_size, 1)             # Estima el valor\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.hx = torch.zeros(1, self.hidden_size)\n",
    "        self.cx = torch.zeros(1, self.hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x es de tamaño (1, input_size)\n",
    "        self.hx, self.cx = self.lstm(x, (self.hx, self.cx))\n",
    "        logits = self.policy_head(self.hx)\n",
    "        value = self.value_head(self.hx)\n",
    "        return logits, value\n",
    "\n",
    "# ======= 3. Crear la entrada del agente =======\n",
    "def get_input(last_action, last_reward, timestep, num_actions=2):\n",
    "    \"\"\"\n",
    "    Construye un vector de entrada para el LSTM a partir de:\n",
    "      - La acción previa (one-hot de dimensión 2)\n",
    "      - La recompensa previa (dimensión 1)\n",
    "      - El timestep normalizado (dimensión 1)\n",
    "    \"\"\"\n",
    "    action_one_hot = F.one_hot(torch.tensor([last_action]), num_classes=num_actions).float()\n",
    "    reward_tensor = torch.tensor([[last_reward]], dtype=torch.float32)\n",
    "    timestep_tensor = torch.tensor([[timestep / 10.0]], dtype=torch.float32)  # Normalizamos para evitar valores altos\n",
    "    x = torch.cat([action_one_hot, reward_tensor, timestep_tensor], dim=1)\n",
    "    #concatena los tensores de manera horizontal (dim = 1)\n",
    "    return x\n",
    "\n",
    "# ======= 4. HIPERPARÁMETROS DE PPO =======\n",
    "gamma = 0.99           # Factor de descuento\n",
    "clip_epsilon = 0.2     # Parámetro de recorte PPO (clip)\n",
    "ppo_epochs = 4         # Número de épocas por actualización\n",
    "lr = 0.009             # Tasa de aprendizaje\n",
    "\n",
    "agent = PPOAgent()\n",
    "optimizer = optim.Adam(agent.parameters(), lr=lr)\n",
    "\n",
    "num_episodes = 1000    # Cantidad total de episodios\n",
    "episode_length = 5     # Pasos por episodio\n",
    "\n",
    "# ======= 5. CICLO DE ENTRENAMIENTO CON PPO =======\n",
    "for episode in range(num_episodes):\n",
    "    # Usamos el entorno RestlessBandit con volatilidad definida (0.1 para probar en este caso)\n",
    "    env = RestlessBandit(volatility=0.1)\n",
    "    agent.reset_state()\n",
    "    \n",
    "    # Listas para almacenar la trayectoria del episodio\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    log_probs = []\n",
    "    values = []\n",
    "    \n",
    "    # Inicializamos: se parte de una acción por defecto (0) y recompensa 0 para el primer paso.\n",
    "    last_action = 0\n",
    "    last_reward = 0.0\n",
    "    \n",
    "    # Recorrido del episodio\n",
    "    for t in range(episode_length):\n",
    "        x = get_input(last_action, last_reward, t)\n",
    "        logits, value = agent(x)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        \n",
    "        # Guardamos los datos de la transición\n",
    "        states.append(x)\n",
    "        actions.append(action)\n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value)\n",
    "        \n",
    "        # Ejecutamos la acción en el entorno y obtenemos la recompensa.\n",
    "        reward = env.pull(action.item())\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        last_action = action.item()\n",
    "        last_reward = reward\n",
    "    \n",
    "    # ======= 5.1. Calcular los RETURNS y las VENTAJAS =======\n",
    "    returns = []\n",
    "    G = 0\n",
    "    for r in reversed(rewards):\n",
    "        G = r + gamma * G\n",
    "        returns.insert(0, G)\n",
    "    returns = torch.tensor(returns, dtype=torch.float32).unsqueeze(1)\n",
    "    values = torch.cat(values)\n",
    "    advantages = returns - values.detach()\n",
    "    \n",
    "    old_log_probs = torch.cat(log_probs).detach()\n",
    "    \n",
    "    # ======= 5.2. Actualización PPO sobre la trayectoria recogida =======\n",
    "    for _ in range(ppo_epochs):\n",
    "        new_log_probs = []\n",
    "        new_values = []\n",
    "        agent.reset_state()  # Reiniciamos el estado para reevaluar la trayectoria almacenada\n",
    "        \n",
    "        # Se reevalúa cada estado almacenado en la trayectoria\n",
    "        for i, x in enumerate(states):\n",
    "            logits, value = agent(x)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            dist = torch.distributions.Categorical(probs)\n",
    "            new_log_probs.append(dist.log_prob(actions[i]))\n",
    "            new_values.append(value)\n",
    "        new_log_probs = torch.cat(new_log_probs)\n",
    "        new_values = torch.cat(new_values)\n",
    "        \n",
    "        ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "        surr1 = ratio * advantages\n",
    "        surr2 = torch.clamp(ratio, 1 - clip_epsilon, 1 + clip_epsilon) * advantages\n",
    "        policy_loss = -torch.min(surr1, surr2).mean()\n",
    "        value_loss = F.mse_loss(new_values, returns)\n",
    "        loss = policy_loss + 0.5 * value_loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if (episode+1) % 100 == 0:\n",
    "        total_reward = sum(rewards)\n",
    "        print(f\"Episode {episode+1}, Total Reward: {total_reward}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"Entrenamiento completado.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Ejemplo de funcionamiento con un agente en modo evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilidades ocultas del entorno: [0.6392585397974725, 0.3607414602025275]\n",
      "Paso 0 | Acción: 0 | Recompensa: 1\n",
      "Paso 1 | Acción: 0 | Recompensa: 1\n",
      "Paso 2 | Acción: 0 | Recompensa: 0\n",
      "Paso 3 | Acción: 0 | Recompensa: 1\n",
      "Paso 4 | Acción: 0 | Recompensa: 1\n",
      "Recompensa total: 4\n"
     ]
    }
   ],
   "source": [
    "# ======= 6. Evaluación del agente entrenado con PPO =======\n",
    "agent.eval()  # Cambia el modelo a modo evaluación\n",
    "env = RestlessBandit(volatility=0.1)  # Nuevo episodio de prueba en entorno cambiante\n",
    "agent.reset_state()\n",
    "\n",
    "last_action = 0\n",
    "last_reward = 0\n",
    "total_reward = 0\n",
    "\n",
    "print(\"Probabilidades ocultas del entorno:\", env.probs)\n",
    "\n",
    "for t in range(5):  # Evaluamos por 5 pasos\n",
    "    with torch.no_grad():\n",
    "        x = get_input(last_action, last_reward, t)\n",
    "        logits, _ = agent(x)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        action = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "    reward = env.pull(action)\n",
    "    total_reward += reward\n",
    "    print(f\"Paso {t} | Acción: {action} | Recompensa: {reward}\")\n",
    "    last_action = action\n",
    "    last_reward = reward\n",
    "\n",
    "print(\"Recompensa total:\", total_reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
