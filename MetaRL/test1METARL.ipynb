{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Lógica de NN para el 2 armed bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# ======= 1. ENTORNO: HierarchicalBandit =======\n",
    "class HierarchicalBandit:\n",
    "    def __init__(self, episode_length, swap1=None, swap2=None):\n",
    "        \"\"\"\n",
    "        Estructura por fases dentro de un mismo episodio:\n",
    "        - Fase 1 (t < swap1): puertas A vs B\n",
    "        - Fase 2 (swap1 <= t < swap2): puertas C vs D\n",
    "        - Fase 3 (t >= swap2): si mode='best', elegir entre la mejor de {A,B} vs la mejor de {C,D};\n",
    "                             si mode='worst', entre la peor de {A,B} vs la peor de {C,D}.\n",
    "        El agente debe inferir en cada fase cuál es la puerta óptima.\n",
    "        \"\"\"\n",
    "        mapping = {\n",
    "            'A': (0.75, 10),\n",
    "            'B': (0.25, 10),\n",
    "            'C': (0.75,  3),\n",
    "            'D': (0.25,  3),\n",
    "        }\n",
    "        # Guardamos las probabilidades y recompensas para cada puerta\n",
    "        self.mapping = mapping\n",
    "\n",
    "        # Determinar puntos de swap\n",
    "        self.episode_length = episode_length\n",
    "        self.swap1 = swap1 if swap1 is not None else np.random.randint(1, episode_length - 2)\n",
    "        self.swap2 = swap2 if swap2 is not None else np.random.randint(self.swap1 + 1, episode_length - 1)\n",
    "\n",
    "        # Modo: 'best' o 'worst'\n",
    "        self.mode = 'best' if np.random.rand() < 0.5 else 'worst'\n",
    "\n",
    "        # Elegir valores de A/B en Fase1 y C/D en Fase2\n",
    "        self.aby_pair = ('A', 'B')\n",
    "        self.cdy_pair = ('C', 'D')\n",
    "\n",
    "        # Calcular cuál es la mejor/peor puerta en AB, y en CD\n",
    "        # “Mejor” = mayor p; “Peor” = menor p\n",
    "        a_p, a_r = mapping['A']\n",
    "        b_p, b_r = mapping['B']\n",
    "        if a_p > b_p:\n",
    "            self.best_ab, self.worst_ab = 'A', 'B'\n",
    "        else:\n",
    "            self.best_ab, self.worst_ab = 'B', 'A'\n",
    "\n",
    "        c_p, c_r = mapping['C']\n",
    "        d_p, d_r = mapping['D']\n",
    "        if c_p > d_p:\n",
    "            self.best_cd, self.worst_cd = 'C', 'D'\n",
    "        else:\n",
    "            self.best_cd, self.worst_cd = 'D', 'C'\n",
    "\n",
    "        # Estado temporal\n",
    "        self.step_count = 0\n",
    "\n",
    "    def pull(self, action):\n",
    "        t = self.step_count\n",
    "\n",
    "        # Fase 1: A vs B\n",
    "        if t < self.swap1:\n",
    "            label = self.aby_pair[action]      # action=0 -> 'A',  action=1 -> 'B'\n",
    "        # Fase 2: C vs D\n",
    "        elif t < self.swap2:\n",
    "            label = self.cdy_pair[action]      # action=0 -> 'C',  action=1 -> 'D'\n",
    "        # Fase 3: best vs best  (o worst vs worst)\n",
    "        else:\n",
    "            if self.mode == 'best':\n",
    "                # action=0 -> puerta best_ab, action=1 -> puerta best_cd\n",
    "                label = self.best_ab if action == 0 else self.best_cd\n",
    "            else:\n",
    "                # action=0 -> puerta worst_ab, action=1 -> puerta worst_cd\n",
    "                label = self.worst_ab if action == 0 else self.worst_cd\n",
    "\n",
    "        # Obtenemos probabilidad y magnitud para “label”\n",
    "        p, r = self.mapping[label]\n",
    "        reward = r if np.random.rand() < p else -r\n",
    "\n",
    "        self.step_count += 1\n",
    "        return reward\n",
    "\n",
    "    def current_info(self):\n",
    "        \"\"\"Información para debug/evaluación: fases y parejas ocultas.\"\"\"\n",
    "        return {\n",
    "            'swap1': self.swap1,\n",
    "            'swap2': self.swap2,\n",
    "            'mode': self.mode,\n",
    "            'best_ab': self.best_ab,\n",
    "            'best_cd': self.best_cd,\n",
    "            'worst_ab': self.worst_ab,\n",
    "            'worst_cd': self.worst_cd\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Entrenamiento con PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 200, Total Reward: 6.6, Loss: 3.71\n",
      "Episode 400, Total Reward: 3.3, Loss: 4.46\n",
      "Episode 600, Total Reward: 3.9, Loss: 0.66\n",
      "Episode 800, Total Reward: 6.6, Loss: 0.11\n",
      "Episode 1000, Total Reward: 4.6, Loss: 3.95\n",
      "Episode 1200, Total Reward: 5.8, Loss: 0.44\n",
      "Episode 1400, Total Reward: -4.8, Loss: 9.13\n",
      "Episode 1600, Total Reward: 4.0, Loss: 1.01\n",
      "Episode 1800, Total Reward: 6.6, Loss: 2.90\n",
      "Episode 2000, Total Reward: 11.3, Loss: 3.36\n",
      "Entrenamiento completado.\n"
     ]
    }
   ],
   "source": [
    "# ======= 2. MODELO: Agente PPO con LSTM =======\n",
    "class PPOAgent(nn.Module):\n",
    "    def __init__(self, input_size=4, hidden_size=32, num_actions=2):\n",
    "        super(PPOAgent, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTMCell(input_size, hidden_size)\n",
    "        self.policy_head = nn.Linear(hidden_size, num_actions)\n",
    "        self.value_head  = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.hx = torch.zeros(1, self.hidden_size)\n",
    "        self.cx = torch.zeros(1, self.hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.hx, self.cx = self.lstm(x, (self.hx, self.cx))\n",
    "        logits = self.policy_head(self.hx)\n",
    "        value  = self.value_head(self.hx)\n",
    "        return logits, value\n",
    "\n",
    "\n",
    "# ======= 3. Crear la entrada del agente =======\n",
    "def get_input(last_action, last_reward, timestep, num_actions=2):\n",
    "    a = F.one_hot(torch.tensor([last_action]), num_classes=num_actions).float()\n",
    "    r = torch.tensor([[last_reward]], dtype=torch.float32)\n",
    "    t = torch.tensor([[timestep / 10.0]], dtype=torch.float32)\n",
    "    return torch.cat([a, r, t], dim=1)\n",
    "\n",
    "\n",
    "# ======= 4. HIPERPARÁMETROS DE PPO =======\n",
    "gamma        = 0.99\n",
    "clip_epsilon = 0.2\n",
    "ppo_epochs   = 4\n",
    "lr           = 0.001\n",
    "\n",
    "agent     = PPOAgent()\n",
    "optimizer = optim.Adam(agent.parameters(), lr=lr)\n",
    "\n",
    "num_episodes   = 2000\n",
    "episode_length = 20\n",
    "\n",
    "\n",
    "# ======= 5. ENTRENAMIENTO CON PPO =======\n",
    "for episode in range(num_episodes):\n",
    "    env = HierarchicalBandit(episode_length=episode_length)\n",
    "    agent.reset_state()\n",
    "\n",
    "    states, actions, rewards, logps, values = [], [], [], [], []\n",
    "    last_action, last_reward = 0, 0.0\n",
    "\n",
    "    for t in range(episode_length):\n",
    "        x      = get_input(last_action, last_reward, t)\n",
    "        logits, v = agent(x)\n",
    "        probs  = F.softmax(logits, dim=1)\n",
    "        dist   = torch.distributions.Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        logp   = dist.log_prob(action)\n",
    "\n",
    "        states.append(x)\n",
    "        actions.append(action)\n",
    "        logps.append(logp)\n",
    "        values.append(v)\n",
    "\n",
    "        reward = env.pull(action.item())\n",
    "        reward = reward / 10\n",
    "        rewards.append(reward)\n",
    "\n",
    "        last_action, last_reward = action.item(), reward\n",
    "\n",
    "    # Calcular returns y ventajas\n",
    "    returns, G = [], 0\n",
    "    for r in reversed(rewards):\n",
    "        G = r + gamma * G\n",
    "        returns.insert(0, G)\n",
    "    returns    = torch.tensor(returns, dtype=torch.float32).unsqueeze(1)\n",
    "    values     = torch.cat(values)\n",
    "    advantages = returns - values.detach()\n",
    "    old_logps  = torch.cat(logps).detach()\n",
    "\n",
    "    # Actualización PPO\n",
    "    for _ in range(ppo_epochs):\n",
    "        new_logps, new_vals, entropies = [], [], []\n",
    "        agent.reset_state()\n",
    "        for i, x in enumerate(states):\n",
    "            logits, v   = agent(x)\n",
    "            probs       = F.softmax(logits, dim=1)\n",
    "            dist        = torch.distributions.Categorical(probs)\n",
    "            new_logps.append(dist.log_prob(actions[i]))\n",
    "            new_vals.append(v)\n",
    "            entropies.append(dist.entropy())\n",
    "        new_logps    = torch.cat(new_logps)\n",
    "        new_vals     = torch.cat(new_vals)\n",
    "        entropy_term = torch.cat(entropies).mean()\n",
    "\n",
    "        ratio       = torch.exp(new_logps - old_logps)\n",
    "        s1          = ratio * advantages\n",
    "        s2          = torch.clamp(ratio, 1-clip_epsilon, 1+clip_epsilon) * advantages\n",
    "        policy_loss = -torch.min(s1, s2).mean()\n",
    "        value_loss  = F.mse_loss(new_vals, returns)\n",
    "        loss        = policy_loss + 0.5*value_loss - 0.01*entropy_term\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (episode + 1) % 200 == 0:\n",
    "        print(f\"Episode {episode+1}, Total Reward: {sum(rewards)}, Loss: {loss.item():.2f}\")\n",
    "\n",
    "print(\"Entrenamiento completado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Modelo guardado en: ppo_lstm_hierarchical.pth\n"
     ]
    }
   ],
   "source": [
    "model_path = \"ppo_lstm_hierarchical.pth\"\n",
    "torch.save(agent.state_dict(), model_path)\n",
    "print(f\" Modelo guardado en: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Ejemplo de funcionamiento con un agente en modo evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swap1=16, swap2=18, mode=best\n",
      "best_ab=A, best_cd=C, worst_ab=B, worst_cd=D\n",
      "\n",
      "Paso  0 | probs_política=[0.9988411068916321, 0.0011589151108637452] | Acción=0 | Recompensa=-10\n",
      "Paso  1 | probs_política=[0.925616443157196, 0.07438356429338455] | Acción=0 | Recompensa=10\n",
      "Paso  2 | probs_política=[0.9999978542327881, 2.1924056454736274e-06] | Acción=0 | Recompensa=10\n",
      "Paso  3 | probs_política=[0.9999991655349731, 7.87527199008764e-07] | Acción=0 | Recompensa=10\n",
      "Paso  4 | probs_política=[0.9999992847442627, 7.002342385931115e-07] | Acción=0 | Recompensa=10\n",
      "Paso  5 | probs_política=[0.9999992847442627, 6.77510058721964e-07] | Acción=0 | Recompensa=10\n",
      "Paso  6 | probs_política=[0.9999992847442627, 7.041181788736139e-07] | Acción=0 | Recompensa=-10\n",
      "Paso  7 | probs_política=[0.9993491768836975, 0.00065076001919806] | Acción=0 | Recompensa=-10\n",
      "Paso  8 | probs_política=[0.9888448119163513, 0.011155142448842525] | Acción=0 | Recompensa=10\n",
      "Paso  9 | probs_política=[0.999997615814209, 2.4349662908207392e-06] | Acción=0 | Recompensa=10\n",
      "Paso 10 | probs_política=[0.9999990463256836, 1.0041147788797389e-06] | Acción=0 | Recompensa=10\n",
      "Paso 11 | probs_política=[0.9999990463256836, 9.623647656553658e-07] | Acción=0 | Recompensa=-10\n",
      "Paso 12 | probs_política=[0.9987291693687439, 0.0012708143331110477] | Acción=0 | Recompensa=10\n",
      "Paso 13 | probs_política=[0.9999988079071045, 1.1331674159009708e-06] | Acción=0 | Recompensa=10\n",
      "Paso 14 | probs_política=[0.9999988079071045, 1.1397482921893243e-06] | Acción=0 | Recompensa=10\n",
      "Paso 15 | probs_política=[0.999998927116394, 1.1257482128712581e-06] | Acción=0 | Recompensa=-10\n",
      "Paso 16 | probs_política=[0.9987026453018188, 0.0012972855474799871] | Acción=0 | Recompensa=-3\n",
      "Paso 17 | probs_política=[0.9956092238426208, 0.004390825517475605] | Acción=0 | Recompensa=3\n",
      "Paso 18 | probs_política=[0.9999550580978394, 4.488841659622267e-05] | Acción=0 | Recompensa=10\n",
      "Paso 19 | probs_política=[0.9999988079071045, 1.2095680403945153e-06] | Acción=0 | Recompensa=10\n",
      "\n",
      "Recompensa total: 80\n"
     ]
    }
   ],
   "source": [
    "# ======= 6. EVALUACIÓN =======\n",
    "agent.eval()\n",
    "env = HierarchicalBandit(episode_length=episode_length)\n",
    "agent.reset_state()\n",
    "\n",
    "info = env.current_info()\n",
    "print(f\"swap1={info['swap1']}, swap2={info['swap2']}, mode={info['mode']}\")\n",
    "print(f\"best_ab={info['best_ab']}, best_cd={info['best_cd']}, worst_ab={info['worst_ab']}, worst_cd={info['worst_cd']}\\n\")\n",
    "\n",
    "last_action, last_reward = 0, 0\n",
    "total_reward = 0\n",
    "\n",
    "for t in range(episode_length):\n",
    "    with torch.no_grad():\n",
    "        x      = get_input(last_action, last_reward, t)\n",
    "        logits, _ = agent(x)\n",
    "        probs  = F.softmax(logits, dim=1)[0].tolist()\n",
    "        action = torch.multinomial(torch.tensor(probs), 1).item()\n",
    "\n",
    "    reward = env.pull(action)\n",
    "    total_reward += reward\n",
    "    print(f\"Paso {t:2d} | probs_política={probs} | Acción={action} | Recompensa={reward}\")\n",
    "    last_action, last_reward = action, reward\n",
    "\n",
    "print(f\"\\nRecompensa total: {total_reward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
