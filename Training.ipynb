{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0b813e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n",
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36960300",
   "metadata": {},
   "source": [
    "## PPO Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dc4177d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from torch import multiprocessing\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tensordict.nn import TensorDictModule\n",
    "from torch import nn\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from tensordict import TensorDict\n",
    "from torchrl.data.replay_buffers import ReplayBuffer\n",
    "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
    "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
    "from torchrl.envs import ParallelEnv, TransformedEnv, Compose, StepCounter \n",
    "from torchrl.envs.transforms import Transform, RemoveEmptySpecs\n",
    "from torchrl.envs.utils import check_env_specs, ExplorationType, set_exploration_type\n",
    "from torchrl.modules import ProbabilisticActor, ValueOperator\n",
    "\n",
    "from torch.distributions import Bernoulli\n",
    "\n",
    "from torchrl.objectives import ClipPPOLoss, ValueEstimators\n",
    "from torchrl.objectives.value import GAE\n",
    "from tqdm import tqdm\n",
    "\n",
    "from Environment.MazeEnv import TorchRLMazeEnv\n",
    "\n",
    "import uuid\n",
    "import os\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd75e3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_fork = multiprocessing.get_start_method() == \"fork\"\n",
    "device = (\n",
    "    torch.device(0)\n",
    "    if torch.cuda.is_available() and not is_fork\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "num_cells = 256  # number of cells in each layer i.e. output dim.\n",
    "lr = 3e-4\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "batches = 1\n",
    "size = 4\n",
    "\n",
    "frames_per_batch = batches * size\n",
    "# For a complete training, bring the number of frames up to 1M\n",
    "total_frames = frames_per_batch * 256\n",
    "\n",
    "sub_batch_size = 32  # cardinality of the sub-samples gathered from the current data in the inner loop\n",
    "num_epochs = 10# optimization steps per batch of data collected\n",
    "clip_epsilon = (\n",
    "    0.2  # clip value for PPO loss: see the equation in the intro for more context.\n",
    ")\n",
    "gamma = 0.99\n",
    "lmbda = 0.95\n",
    "entropy_eps = 1e-4\n",
    "\n",
    "# Environment hyper-parameters\n",
    "env_width = 128\n",
    "env_height = 72\n",
    "\n",
    "filename = \"\"\n",
    "\n",
    "runID = uuid.uuid4()\n",
    "\n",
    "runDir = f'./checkpoints/{runID}'\n",
    "\n",
    "os.mkdir(runDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af235e9",
   "metadata": {},
   "source": [
    "### Creating the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b586b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TorchRLMazeEnv(width=env_width, height=env_height, batches=batches, size=size, device=device, show_windows=True)\n",
    "\n",
    "# Add transforms in the correct order\n",
    "env = TransformedEnv(env, Compose( RemoveEmptySpecs(), StepCounter() ))\n",
    "\n",
    "# check_env_specs(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b01750",
   "metadata": {},
   "source": [
    "### Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1b51dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNet(nn.Module):\n",
    "    def __init__(self, num_actions, num_cells=256):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=8, stride=4, device=device),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, device=device),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, device=device),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(start_dim=1),\n",
    "        )\n",
    "        \n",
    "        # Calculate the flattened size properly\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, 3, env_height, env_width, device=device)  # Add batch dimension\n",
    "            n_flat = self.features(dummy).shape[1]  # Get the flattened size\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(n_flat, num_cells, device=device),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_cells, num_actions, device=device),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x.shape) == 3:\n",
    "            x = x.unsqueeze(0)\n",
    "        x = self.features(x)\n",
    "        probs = self.head(x)\n",
    "        if len(probs.shape) == 3:\n",
    "            probs = probs.squeeze(0)\n",
    "        return probs\n",
    "\n",
    "# Create the actor and value operators as before\n",
    "actor = TensorDictModule(\n",
    "    module=ActorNet(6),\n",
    "    in_keys=[\"observation\"],\n",
    "    out_keys=[\"logits\"],\n",
    ")\n",
    "\n",
    "actor = ProbabilisticActor(\n",
    "    module=actor,\n",
    "    spec=env.action_spec,\n",
    "    in_keys=[\"logits\"],\n",
    "    distribution_class=Bernoulli,\n",
    "    distribution_kwargs={},\n",
    "    return_log_prob=True,\n",
    "    out_keys=[\"action\"],\n",
    ")\n",
    "\n",
    "value = ValueOperator(\n",
    "    module=ActorNet(1),\n",
    "    in_keys=[\"observation\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f436a862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        action: Tensor(shape=torch.Size([4, 1, 6]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
       "        collector: TensorDict(\n",
       "            fields={\n",
       "                traj_ids: Tensor(shape=torch.Size([4, 1]), device=cuda:0, dtype=torch.int64, is_shared=True)},\n",
       "            batch_size=torch.Size([4, 1]),\n",
       "            device=cuda:0,\n",
       "            is_shared=True),\n",
       "        done: Tensor(shape=torch.Size([4, 1, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
       "        logits: Tensor(shape=torch.Size([4, 1, 6]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
       "        next: TensorDict(\n",
       "            fields={\n",
       "                done: Tensor(shape=torch.Size([4, 1, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
       "                observation: Tensor(shape=torch.Size([4, 1, 3, 72, 128]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
       "                reward: Tensor(shape=torch.Size([4, 1, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
       "                step_count: Tensor(shape=torch.Size([4, 1, 1]), device=cuda:0, dtype=torch.int64, is_shared=True),\n",
       "                terminated: Tensor(shape=torch.Size([4, 1, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
       "            batch_size=torch.Size([4, 1]),\n",
       "            device=cuda:0,\n",
       "            is_shared=True),\n",
       "        observation: Tensor(shape=torch.Size([4, 1, 3, 72, 128]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
       "        sample_log_prob: Tensor(shape=torch.Size([4, 1, 6]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
       "        step_count: Tensor(shape=torch.Size([4, 1, 1]), device=cuda:0, dtype=torch.int64, is_shared=True),\n",
       "        terminated: Tensor(shape=torch.Size([4, 1, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
       "    batch_size=torch.Size([4, 1]),\n",
       "    device=cuda:0,\n",
       "    is_shared=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collector = SyncDataCollector(\n",
    "    env,\n",
    "    actor,\n",
    "    frames_per_batch=frames_per_batch,\n",
    "    total_frames=total_frames,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "replay_buffer = ReplayBuffer(\n",
    "    storage=LazyTensorStorage(max_size=frames_per_batch, device=device),\n",
    "    sampler=SamplerWithoutReplacement(),\n",
    "    batch_size=torch.Size([frames_per_batch]),\n",
    ")\n",
    "\n",
    "advantage_module = GAE(\n",
    "    gamma=gamma, lmbda=lmbda, value_network=value, average_gae=True, device=device,\n",
    ")\n",
    "\n",
    "loss_module = ClipPPOLoss(\n",
    "    actor_network=actor,\n",
    "    critic_network=value,\n",
    "    clip_epsilon=clip_epsilon,\n",
    "    entropy_bonus=bool(entropy_eps),\n",
    "    entropy_coef=entropy_eps,\n",
    "    critic_coef=1.0,\n",
    "    loss_critic_type=\"smooth_l1\",\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "optim = torch.optim.Adam(loss_module.parameters(), lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optim, total_frames // frames_per_batch, 0.0\n",
    ")\n",
    "\n",
    "collector.rollout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375cea7d",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f654737",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1024 [00:30<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 85\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i % \u001b[32m10\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m     77\u001b[39m     \u001b[38;5;66;03m# We evaluate the policy once every 10 batches of data.\u001b[39;00m\n\u001b[32m     78\u001b[39m     \u001b[38;5;66;03m# Evaluation is rather simple: execute the policy without exploration\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     81\u001b[39m     \u001b[38;5;66;03m# The ``rollout`` method of the ``env`` can take a policy as argument:\u001b[39;00m\n\u001b[32m     82\u001b[39m     \u001b[38;5;66;03m# it will then execute this policy at each step.\u001b[39;00m\n\u001b[32m     83\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_exploration_type(ExplorationType.DETERMINISTIC), torch.no_grad():\n\u001b[32m     84\u001b[39m         \u001b[38;5;66;03m# execute a rollout with the trained policy\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m         eval_rollout = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrollout\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m         logs[\u001b[33m\"\u001b[39m\u001b[33meval reward\u001b[39m\u001b[33m\"\u001b[39m].append(eval_rollout[\u001b[33m\"\u001b[39m\u001b[33mnext\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mreward\u001b[39m\u001b[33m\"\u001b[39m].mean().item())\n\u001b[32m     87\u001b[39m         logs[\u001b[33m\"\u001b[39m\u001b[33meval reward (sum)\u001b[39m\u001b[33m\"\u001b[39m].append(\n\u001b[32m     88\u001b[39m             eval_rollout[\u001b[33m\"\u001b[39m\u001b[33mnext\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mreward\u001b[39m\u001b[33m\"\u001b[39m].sum().item()\n\u001b[32m     89\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/choice-playgrounds/.venv/lib/python3.12/site-packages/torchrl/envs/common.py:3307\u001b[39m, in \u001b[36mEnvBase.rollout\u001b[39m\u001b[34m(self, max_steps, policy, callback, auto_reset, auto_cast_to_device, break_when_any_done, break_when_all_done, return_contiguous, tensordict, set_truncated, out, trust_policy)\u001b[39m\n\u001b[32m   3297\u001b[39m kwargs = {\n\u001b[32m   3298\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtensordict\u001b[39m\u001b[33m\"\u001b[39m: tensordict,\n\u001b[32m   3299\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mauto_cast_to_device\u001b[39m\u001b[33m\"\u001b[39m: auto_cast_to_device,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3304\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcallback\u001b[39m\u001b[33m\"\u001b[39m: callback,\n\u001b[32m   3305\u001b[39m }\n\u001b[32m   3306\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m break_when_any_done \u001b[38;5;129;01mor\u001b[39;00m break_when_all_done:\n\u001b[32m-> \u001b[39m\u001b[32m3307\u001b[39m     tensordicts = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_rollout_stop_early\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3308\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbreak_when_all_done\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbreak_when_all_done\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3309\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbreak_when_any_done\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbreak_when_any_done\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3310\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3311\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3312\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3313\u001b[39m     tensordicts = \u001b[38;5;28mself\u001b[39m._rollout_nonstop(**kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/choice-playgrounds/.venv/lib/python3.12/site-packages/torchrl/envs/common.py:3448\u001b[39m, in \u001b[36mEnvBase._rollout_stop_early\u001b[39m\u001b[34m(self, break_when_any_done, break_when_all_done, tensordict, auto_cast_to_device, max_steps, policy, policy_device, env_device, callback)\u001b[39m\n\u001b[32m   3446\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3447\u001b[39m         tensordict.clear_device_()\n\u001b[32m-> \u001b[39m\u001b[32m3448\u001b[39m tensordict = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3449\u001b[39m td_append = tensordict.copy()\n\u001b[32m   3450\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m break_when_all_done:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/choice-playgrounds/.venv/lib/python3.12/site-packages/torchrl/envs/common.py:2031\u001b[39m, in \u001b[36mEnvBase.step\u001b[39m\u001b[34m(self, tensordict)\u001b[39m\n\u001b[32m   2028\u001b[39m next_preset = tensordict.get(\u001b[33m\"\u001b[39m\u001b[33mnext\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m   2030\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m next_tensordict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2031\u001b[39m     next_tensordict = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2032\u001b[39m     next_tensordict = \u001b[38;5;28mself\u001b[39m._step_proc_data(next_tensordict)\n\u001b[32m   2033\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m next_preset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2034\u001b[39m     \u001b[38;5;66;03m# tensordict could already have a \"next\" key\u001b[39;00m\n\u001b[32m   2035\u001b[39m     \u001b[38;5;66;03m# this could be done more efficiently by not excluding but just passing\u001b[39;00m\n\u001b[32m   2036\u001b[39m     \u001b[38;5;66;03m# the necessary keys\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/choice-playgrounds/.venv/lib/python3.12/site-packages/torchrl/envs/transforms/transforms.py:1129\u001b[39m, in \u001b[36mTransformedEnv._step\u001b[39m\u001b[34m(self, tensordict)\u001b[39m\n\u001b[32m   1126\u001b[39m         tensordict_batch_size = \u001b[38;5;28mself\u001b[39m.batch_size\n\u001b[32m   1128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m next_tensordict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1129\u001b[39m     next_tensordict = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_env\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict_in\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1130\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m next_preset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1131\u001b[39m         \u001b[38;5;66;03m# tensordict could already have a \"next\" key\u001b[39;00m\n\u001b[32m   1132\u001b[39m         \u001b[38;5;66;03m# this could be done more efficiently by not excluding but just passing\u001b[39;00m\n\u001b[32m   1133\u001b[39m         \u001b[38;5;66;03m# the necessary keys\u001b[39;00m\n\u001b[32m   1134\u001b[39m         next_tensordict.update(\n\u001b[32m   1135\u001b[39m             next_preset.exclude(*next_tensordict.keys(\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[32m   1136\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/choice-playgrounds/Environment/MazeEnv.py:757\u001b[39m, in \u001b[36mTorchRLMazeEnv._step\u001b[39m\u001b[34m(self, tensordict)\u001b[39m\n\u001b[32m    754\u001b[39m \u001b[38;5;66;03m# Convert to native actions\u001b[39;00m\n\u001b[32m    755\u001b[39m native_actions = actions.cpu().numpy()\n\u001b[32m--> \u001b[39m\u001b[32m757\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnative_actions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_count\u001b[49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# a list of N = batches*size tuples\u001b[39;00m\n\u001b[32m    758\u001b[39m imgs, rewards, dones = \u001b[38;5;28mzip\u001b[39m(*results)      \u001b[38;5;66;03m# each is a length-N tuple\u001b[39;00m\n\u001b[32m    760\u001b[39m obs    = torch.stack([torch.tensor(img)   \u001b[38;5;28;01mfor\u001b[39;00m img   \u001b[38;5;129;01min\u001b[39;00m imgs],    dim=\u001b[32m0\u001b[39m).to(\u001b[38;5;28mself\u001b[39m.device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/choice-playgrounds/Environment/MazeEnv.py:111\u001b[39m, in \u001b[36mmultiGames.step\u001b[39m\u001b[34m(self, inputs, stepCount)\u001b[39m\n\u001b[32m    108\u001b[39m results = [\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.batches)]\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.batches):\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     \u001b[43mthreadWorker\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgames\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstepCount\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[38;5;66;03m# for i in range(self.batches):\u001b[39;00m\n\u001b[32m    120\u001b[39m \u001b[38;5;66;03m#     t = threading.Thread(\u001b[39;00m\n\u001b[32m    121\u001b[39m \u001b[38;5;66;03m#         target=threadWorker,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    134\u001b[39m \n\u001b[32m    135\u001b[39m \u001b[38;5;66;03m# flatten\u001b[39;00m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [item \u001b[38;5;28;01mfor\u001b[39;00m sublist \u001b[38;5;129;01min\u001b[39;00m results \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m sublist]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/choice-playgrounds/Environment/MazeEnv.py:80\u001b[39m, in \u001b[36mthreadWorker\u001b[39m\u001b[34m(wrappers, actions, results, index, stepCount)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mthreadWorker\u001b[39m(wrappers: \u001b[38;5;28mlist\u001b[39m[\u001b[33m'\u001b[39m\u001b[33mGameWrapper\u001b[39m\u001b[33m'\u001b[39m], actions: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mbool\u001b[39m]], results: \u001b[38;5;28mlist\u001b[39m, index: \u001b[38;5;28mint\u001b[39m, stepCount: \u001b[38;5;28mint\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     results[index] = [\u001b[43mw\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstepCount\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m w, a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(wrappers, actions)]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/choice-playgrounds/Environment/MazeEnv.py:597\u001b[39m, in \u001b[36mGameWrapper.step\u001b[39m\u001b[34m(self, actions, stepCount)\u001b[39m\n\u001b[32m    594\u001b[39m \u001b[38;5;28mself\u001b[39m._run_scheduler()\n\u001b[32m    596\u001b[39m \u001b[38;5;66;03m# Get the image of the game\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m597\u001b[39m image = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgame\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_screenshot\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    598\u001b[39m done = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    600\u001b[39m base_progress = \u001b[38;5;28mself\u001b[39m.game.z - \u001b[32m1.5\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/choice-playgrounds/Environment/MazeEnv.py:521\u001b[39m, in \u001b[36mMazeGame.get_screenshot\u001b[39m\u001b[34m(self, save, name)\u001b[39m\n\u001b[32m    519\u001b[39m \u001b[38;5;66;03m# Get the buffer and convert to numpy array\u001b[39;00m\n\u001b[32m    520\u001b[39m buffer = pyglet.image.get_buffer_manager().get_color_buffer()\n\u001b[32m--> \u001b[39m\u001b[32m521\u001b[39m image_data = \u001b[43mbuffer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_image_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    522\u001b[39m width, height = buffer.width, buffer.height\n\u001b[32m    523\u001b[39m pitch = -(image_data.width * \u001b[38;5;28mlen\u001b[39m(image_data.format))  \u001b[38;5;66;03m# Flip vertically\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/choice-playgrounds/.venv/lib/python3.12/site-packages/pyglet/image/__init__.py:2049\u001b[39m, in \u001b[36mBufferImage.get_image_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2047\u001b[39m glPushClientAttrib(GL_CLIENT_PIXEL_STORE_BIT)\n\u001b[32m   2048\u001b[39m glPixelStorei(GL_PACK_ALIGNMENT, \u001b[32m1\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2049\u001b[39m \u001b[43mglReadPixels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2050\u001b[39m \u001b[43m             \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgl_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mGL_UNSIGNED_BYTE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2051\u001b[39m glPopClientAttrib()\n\u001b[32m   2053\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ImageData(\u001b[38;5;28mself\u001b[39m.width, \u001b[38;5;28mself\u001b[39m.height, \u001b[38;5;28mself\u001b[39m.format, buffer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/choice-playgrounds/.venv/lib/python3.12/site-packages/pyglet/gl/lib.py:87\u001b[39m, in \u001b[36merrcheck\u001b[39m\u001b[34m(result, func, arguments)\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mGLException\u001b[39;00m(\u001b[38;5;167;01mException\u001b[39;00m):\n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34merrcheck\u001b[39m(result, func, arguments):\n\u001b[32m     88\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _debug_gl_trace:\n\u001b[32m     89\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "logs = defaultdict(list)\n",
    "pbar = tqdm(total=total_frames)\n",
    "eval_str = \"\"\n",
    "\n",
    "for i, tensordict_data in enumerate(collector):\n",
    "\n",
    "    tensordict_data = tensordict_data.squeeze(1) # Remove extra added dimension\n",
    "\n",
    "\n",
    "\n",
    "    # we now have a batch of data to work with. Let's learn something from it.\n",
    "    for _ in range(num_epochs):\n",
    "        # We'll need an \"advantage\" signal to make PPO work.\n",
    "        # We re-compute it at each epoch as its value depends on the value\n",
    "        # network which is updated in the inner loop.\n",
    "        advantage_module(tensordict_data)\n",
    "\n",
    "        flat_bs = tensordict_data.batch_size[0]                          # e.g. 64\n",
    "\n",
    "        # 2) squeeze away any extra dims on obs first\n",
    "        #    (you said your obs was already [batch, C, H, W] here)\n",
    "        obs = tensordict_data[\"observation\"]              # [T, N, C, H, W]\n",
    "        obs = obs.reshape(flat_bs, *obs.shape[1:])       # [flat_bs, C, H, W]\n",
    "\n",
    "        # 3) grab event-shapes for the others\n",
    "        action_shape    = tensordict_data[\"action\"].shape[1:]          # e.g. [6]\n",
    "        logprob_shape   = tensordict_data[\"sample_log_prob\"].shape[1:] # e.g. [6]\n",
    "        # state_value, value_target, advantage are scalar => shape []\n",
    "\n",
    "        # 4) rebuild\n",
    "        flat_td = TensorDict(\n",
    "            {\n",
    "            \"observation\":    obs,\n",
    "            \"action\":         tensordict_data[\"action\"]\n",
    "                                    .reshape(flat_bs, *action_shape),\n",
    "            \"sample_log_prob\": tensordict_data[\"sample_log_prob\"]\n",
    "                                    .reshape(flat_bs, *logprob_shape),\n",
    "            \"state_value\":    tensordict_data[\"state_value\"]\n",
    "                                    .reshape(flat_bs, 1),\n",
    "            \"value_target\":   tensordict_data[\"value_target\"]\n",
    "                                    .reshape(flat_bs, 1),\n",
    "            \"advantage\":      tensordict_data[\"advantage\"]\n",
    "                                    .reshape(flat_bs),      # scalar event\n",
    "            },\n",
    "            batch_size=torch.Size([flat_bs]),\n",
    "            device=tensordict_data.device,\n",
    "        )\n",
    "        replay_buffer.extend(flat_td.cpu())\n",
    "\n",
    "        for _ in range(frames_per_batch // sub_batch_size):\n",
    "            subdata = replay_buffer.sample(sub_batch_size).to(device)\n",
    "            loss_vals = loss_module(subdata)\n",
    "            loss_value = (\n",
    "                loss_vals[\"loss_objective\"]\n",
    "                + loss_vals[\"loss_critic\"]\n",
    "                + loss_vals[\"loss_entropy\"]\n",
    "            )\n",
    "\n",
    "            # Optimization: backward, grad clipping and optimization step\n",
    "            loss_value.backward()\n",
    "            # this is not strictly mandatory but it's good practice to keep\n",
    "            # your gradient norm bounded\n",
    "            torch.nn.utils.clip_grad_norm_(loss_module.parameters(), max_grad_norm)\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "    logs[\"reward\"].append(tensordict_data[\"next\", \"reward\"].mean().item())\n",
    "    pbar.update(tensordict_data.numel())\n",
    "    cum_reward_str = (\n",
    "        f\"average reward={logs['reward'][-1]: 4.4f} (init={logs['reward'][0]: 4.4f})\"\n",
    "    )\n",
    "    logs[\"step_count\"].append(tensordict_data[\"step_count\"].max().item())\n",
    "    stepcount_str = f\"step count (max): {logs['step_count'][-1]}\"\n",
    "    logs[\"lr\"].append(optim.param_groups[0][\"lr\"])\n",
    "    lr_str = f\"lr policy: {logs['lr'][-1]: 4.4f}\"\n",
    "    if i % 10 == 0:\n",
    "        # We evaluate the policy once every 10 batches of data.\n",
    "        # Evaluation is rather simple: execute the policy without exploration\n",
    "        # (take the expected value of the action distribution) for a given\n",
    "        # number of steps (1000, which is our ``env`` horizon).\n",
    "        # The ``rollout`` method of the ``env`` can take a policy as argument:\n",
    "        # it will then execute this policy at each step.\n",
    "        with set_exploration_type(ExplorationType.DETERMINISTIC), torch.no_grad():\n",
    "            # execute a rollout with the trained policy\n",
    "            eval_rollout = env.rollout(256, actor)\n",
    "            logs[\"eval reward\"].append(eval_rollout[\"next\", \"reward\"].mean().item())\n",
    "            logs[\"eval reward (sum)\"].append(\n",
    "                eval_rollout[\"next\", \"reward\"].sum().item()\n",
    "            )\n",
    "            logs[\"eval step_count\"].append(eval_rollout[\"step_count\"].max().item())\n",
    "            eval_str = (\n",
    "                f\"eval cumulative reward: {logs['eval reward (sum)'][-1]: 4.4f} \"\n",
    "                f\"(init: {logs['eval reward (sum)'][0]: 4.4f}), \"\n",
    "                f\"eval step-count: {logs['eval step_count'][-1]}\"\n",
    "            )\n",
    "            del eval_rollout\n",
    "    pbar.set_description(\", \".join([eval_str, cum_reward_str, stepcount_str, lr_str]))\n",
    "\n",
    "    if i % (frames_per_batch * 8) == 0:\n",
    "        filename = f'{runDir}/{i}.ch'\n",
    "        checkpoint = {\n",
    "            'model_state_dict': actor.module.state_dict(),\n",
    "            'reward': logs[\"eval reward\"][-1],  # Assuming you have an optimizer\n",
    "            'epoch': i,  # Assuming you have an epoch counter\n",
    "        }\n",
    "\n",
    "        torch.save(checkpoint, filename)\n",
    "        \n",
    "    # We're also using a learning rate scheduler. Like the gradient clipping,\n",
    "    # this is a nice-to-have but nothing necessary for PPO to work.\n",
    "    scheduler.step()\n",
    "\n",
    "filename = f'{runDir}/_final.ch'\n",
    "checkpoint = {\n",
    "    'model_state_dict': actor.module.state_dict(),\n",
    "    'reward': logs[\"eval reward\"][-1],  # Assuming you have an optimizer\n",
    "    'epoch': i,  # Assuming you have an epoch counter\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aca7b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{runDir}/logs.json', 'w') as f:\n",
    "    f.write(json.dumps(logs))\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(logs[\"reward\"])\n",
    "plt.title(\"training rewards (average)\")\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(logs[\"step_count\"])\n",
    "plt.title(\"Max step count (training)\")\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(logs[\"eval reward (sum)\"])\n",
    "plt.title(\"Return (test)\")\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(logs[\"eval step_count\"])\n",
    "plt.title(\"Max step count (test)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23840d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model for testing\n",
    "if filename == \"\":\n",
    "    filename = \"checkpoints/47d6adb5-3abd-4640-8b31-50a1b174142d/_final.ch\"\n",
    "\n",
    "l_checkpoint = torch.load(filename)\n",
    "\n",
    "actor.module.state_dict(l_checkpoint[\"model_state_dict\"])\n",
    "\n",
    "env = TorchRLMazeEnv(width=env_width, height=env_height, batches=1, size=1, device=device, show_windows=True)\n",
    "env = TransformedEnv(env, Compose( RemoveEmptySpecs(), StepCounter() ))\n",
    "\n",
    "env.rollout(512, actor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
