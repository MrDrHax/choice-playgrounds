{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b813e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36960300",
   "metadata": {},
   "source": [
    "## PPO Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc4177d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from torch import multiprocessing\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tensordict.nn import TensorDictModule\n",
    "from tensordict.nn.distributions import NormalParamExtractor\n",
    "from torch import nn\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.data.replay_buffers import ReplayBuffer\n",
    "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
    "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
    "from torchrl.envs import (Compose, DoubleToFloat, ObservationNorm, StepCounter, TransformedEnv)\n",
    "from torchrl.envs.libs.gym import GymEnv\n",
    "from torchrl.envs.utils import check_env_specs, ExplorationType, set_exploration_type\n",
    "from torchrl.envs.transforms import UnsqueezeTransform\n",
    "from torchrl.modules import ProbabilisticActor, TanhNormal, ValueOperator\n",
    "from torchrl.objectives import ClipPPOLoss\n",
    "from torchrl.objectives.value import GAE\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd75e3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_fork = multiprocessing.get_start_method() == \"fork\"\n",
    "device = (\n",
    "    torch.device(0)\n",
    "    if torch.cuda.is_available() and not is_fork\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "num_cells = 256  # number of cells in each layer i.e. output dim.\n",
    "lr = 3e-4\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "frames_per_batch = 32\n",
    "# For a complete training, bring the number of frames up to 1M\n",
    "total_frames = 50_000\n",
    "\n",
    "sub_batch_size = 64  # cardinality of the sub-samples gathered from the current data in the inner loop\n",
    "num_epochs = 10  # optimization steps per batch of data collected\n",
    "clip_epsilon = (\n",
    "    0.2  # clip value for PPO loss: see the equation in the intro for more context.\n",
    ")\n",
    "gamma = 0.99\n",
    "lmbda = 0.95\n",
    "entropy_eps = 1e-4\n",
    "\n",
    "# Environment hyper-parameters\n",
    "env_width = 64\n",
    "env_height = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af235e9",
   "metadata": {},
   "source": [
    "### Creating the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b586b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Environment.MazeEnv import TorchRLMazeEnv\n",
    "\n",
    "env = TorchRLMazeEnv(width=env_width, height=env_height, device=device)\n",
    "\n",
    "env = TransformedEnv(\n",
    "    env,\n",
    "    Compose(\n",
    "        StepCounter(),\n",
    "    ),\n",
    ")\n",
    "\n",
    "check_env_specs(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c093d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout = env.rollout(3)\n",
    "print(\"rollout of three steps:\", rollout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b01750",
   "metadata": {},
   "source": [
    "### Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b51dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actor network - CNN based policy network\n",
    "actor_net = nn.Sequential(\n",
    "    # CNN layers\n",
    "    nn.Conv2d(3, 32, kernel_size=8, stride=4, device=device),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(32, 64, kernel_size=4, stride=2, device=device),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(64, 64, kernel_size=3, stride=1, device=device),\n",
    "    nn.ReLU(),\n",
    "    nn.Flatten(),\n",
    "\n",
    "    # Fully connected layers\n",
    "    nn.LazyLinear(512, 512, device=device),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 6, device=device),  # Output 6 logits for binary actions\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "\n",
    "# Create the actor\n",
    "actor = TensorDictModule(\n",
    "    actor_net,\n",
    "    in_keys=[\"observation\"],\n",
    "    out_keys=[\"action\"],\n",
    ")\n",
    "\n",
    "# Value network - Similar architecture but outputs a single value\n",
    "value_net = nn.Sequential(\n",
    "    # CNN layers\n",
    "    nn.Conv2d(3, 32, kernel_size=8, stride=4, device=device),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(3, 64, kernel_size=4, stride=2, device=device),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(3, 64, kernel_size=3, stride=1, device=device),\n",
    "    nn.ReLU(),\n",
    "    nn.Flatten(),\n",
    "\n",
    "    # Fully connected layers\n",
    "    nn.Linear(512, 512, device=device),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 1, device=device),  # Output a single value\n",
    ")\n",
    "\n",
    "# Create the value operator\n",
    "value = ValueOperator(\n",
    "    value_net,\n",
    "    in_keys=[\"observation\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f436a862",
   "metadata": {},
   "outputs": [],
   "source": [
    "collector = SyncDataCollector(\n",
    "    env,\n",
    "    actor,\n",
    "    frames_per_batch=frames_per_batch,\n",
    "    total_frames=total_frames,\n",
    "    split_trajs=False,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "replay_buffer = ReplayBuffer(\n",
    "    storage=LazyTensorStorage(max_size=frames_per_batch),\n",
    "    sampler=SamplerWithoutReplacement(),\n",
    ")\n",
    "\n",
    "advantage_module = GAE(\n",
    "    gamma=gamma, lmbda=lmbda, value_network=value, average_gae=True, device=device,\n",
    ")\n",
    "\n",
    "loss_module = ClipPPOLoss(\n",
    "    actor_network=actor,\n",
    "    critic_network=value,\n",
    "    clip_epsilon=clip_epsilon,\n",
    "    entropy_bonus=bool(entropy_eps),\n",
    "    entropy_coef=entropy_eps,\n",
    "    # these keys match by default but we set this for completeness\n",
    "    critic_coef=1.0,\n",
    "    loss_critic_type=\"smooth_l1\",\n",
    ")\n",
    "\n",
    "optim = torch.optim.Adam(loss_module.parameters(), lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optim, total_frames // frames_per_batch, 0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d42e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with collector\n",
    "\n",
    "for tensordict_data in collector:\n",
    "    print(\"tensordict_data\", tensordict_data)\n",
    "\n",
    "    # Compute the advantages\n",
    "    tensordict_data = advantage_module(tensordict_data)\n",
    "\n",
    "    # Add the data to the replay buffer\n",
    "    replay_buffer.extend(tensordict_data)\n",
    "\n",
    "    # Sample a batch from the replay buffer\n",
    "    batch = replay_buffer.sample(sub_batch_size)\n",
    "\n",
    "    # Optimize the model\n",
    "    for _ in range(num_epochs):\n",
    "        loss = loss_module(batch)\n",
    "        optim.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(loss_module.parameters(), max_grad_norm)\n",
    "        optim.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    print(f\"Loss: {loss.item()}\")  # Print the loss for monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee295b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(logs[\"reward\"])\n",
    "plt.title(\"training rewards (average)\")\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(logs[\"step_count\"])\n",
    "plt.title(\"Max step count (training)\")\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(logs[\"eval reward (sum)\"])\n",
    "plt.title(\"Return (test)\")\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(logs[\"eval step_count\"])\n",
    "plt.title(\"Max step count (test)\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
