{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b813e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36960300",
   "metadata": {},
   "source": [
    "## PPO Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc4177d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from torch import multiprocessing\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tensordict.nn import TensorDictModule\n",
    "from torch import nn\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from tensordict import TensorDict\n",
    "from torchrl.data.replay_buffers import ReplayBuffer\n",
    "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
    "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
    "from torchrl.envs import ParallelEnv, TransformedEnv, Compose, StepCounter \n",
    "from torchrl.envs.transforms import Transform, RemoveEmptySpecs\n",
    "from torchrl.envs.utils import check_env_specs, ExplorationType, set_exploration_type\n",
    "from torchrl.modules import ProbabilisticActor, ValueOperator\n",
    "\n",
    "from torch.distributions import Bernoulli\n",
    "\n",
    "from torchrl.objectives import ClipPPOLoss, ValueEstimators\n",
    "from torchrl.objectives.value import GAE\n",
    "from tqdm import tqdm\n",
    "\n",
    "from Environment.MazeEnv import TorchRLMazeEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd75e3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_fork = multiprocessing.get_start_method() == \"fork\"\n",
    "device = (\n",
    "    torch.device(0)\n",
    "    if torch.cuda.is_available() and not is_fork\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "num_cells = 256  # number of cells in each layer i.e. output dim.\n",
    "lr = 3e-4\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "batches = 1\n",
    "size = 4\n",
    "\n",
    "frames_per_batch = batches * size\n",
    "# For a complete training, bring the number of frames up to 1M\n",
    "total_frames = 50_000\n",
    "\n",
    "sub_batch_size = 4  # cardinality of the sub-samples gathered from the current data in the inner loop\n",
    "num_epochs = 10# optimization steps per batch of data collected\n",
    "clip_epsilon = (\n",
    "    0.2  # clip value for PPO loss: see the equation in the intro for more context.\n",
    ")\n",
    "gamma = 0.99\n",
    "lmbda = 0.95\n",
    "entropy_eps = 1e-4\n",
    "\n",
    "# Environment hyper-parameters\n",
    "env_width = 128\n",
    "env_height = 72"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af235e9",
   "metadata": {},
   "source": [
    "### Creating the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b586b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-26 18:02:29,796 [torchrl][INFO] check_env_specs succeeded!\n"
     ]
    }
   ],
   "source": [
    "env = TorchRLMazeEnv(width=env_width, height=env_height, batches=batches, size=size, device=device, show_windows=True)\n",
    "\n",
    "# Add transforms in the correct order\n",
    "env = TransformedEnv(env, Compose( RemoveEmptySpecs(), StepCounter() ))\n",
    "\n",
    "check_env_specs(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b01750",
   "metadata": {},
   "source": [
    "### Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b51dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNet(nn.Module):\n",
    "    def __init__(self, num_actions, num_cells=256):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(start_dim=1),\n",
    "        )\n",
    "        \n",
    "        # Calculate the flattened size properly\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, 3, env_height, env_width)  # Add batch dimension\n",
    "            n_flat = self.features(dummy).shape[1]  # Get the flattened size\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(n_flat, num_cells),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_cells, num_actions),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x.shape) == 3:\n",
    "            x = x.unsqueeze(0)\n",
    "        x = self.features(x)\n",
    "        probs = self.head(x)\n",
    "        if len(probs.shape) == 3:\n",
    "            probs = probs.squeeze(0)\n",
    "        return probs\n",
    "\n",
    "# Create the actor and value operators as before\n",
    "actor = TensorDictModule(\n",
    "    module=ActorNet(6),\n",
    "    in_keys=[\"observation\"],\n",
    "    out_keys=[\"logits\"],\n",
    ")\n",
    "\n",
    "actor = ProbabilisticActor(\n",
    "    module=actor,\n",
    "    spec=env.action_spec,\n",
    "    in_keys=[\"logits\"],\n",
    "    distribution_class=Bernoulli,\n",
    "    distribution_kwargs={},\n",
    "    return_log_prob=True,\n",
    "    out_keys=[\"action\"],\n",
    ")\n",
    "\n",
    "value = ValueOperator(\n",
    "    module=ActorNet(1),\n",
    "    in_keys=[\"observation\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f436a862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        action: Tensor(shape=torch.Size([128, 1, 6]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        collector: TensorDict(\n",
       "            fields={\n",
       "                traj_ids: Tensor(shape=torch.Size([128, 1]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
       "            batch_size=torch.Size([128, 1]),\n",
       "            device=cpu,\n",
       "            is_shared=False),\n",
       "        done: Tensor(shape=torch.Size([128, 1, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        logits: Tensor(shape=torch.Size([128, 1, 6]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        next: TensorDict(\n",
       "            fields={\n",
       "                done: Tensor(shape=torch.Size([128, 1, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                observation: Tensor(shape=torch.Size([128, 1, 3, 72, 128]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                reward: Tensor(shape=torch.Size([128, 1, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                step_count: Tensor(shape=torch.Size([128, 1, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "                terminated: Tensor(shape=torch.Size([128, 1, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "            batch_size=torch.Size([128, 1]),\n",
       "            device=cpu,\n",
       "            is_shared=False),\n",
       "        observation: Tensor(shape=torch.Size([128, 1, 3, 72, 128]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        sample_log_prob: Tensor(shape=torch.Size([128, 1, 6]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        step_count: Tensor(shape=torch.Size([128, 1, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        terminated: Tensor(shape=torch.Size([128, 1, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "    batch_size=torch.Size([128, 1]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collector = SyncDataCollector(\n",
    "    env,\n",
    "    actor,\n",
    "    frames_per_batch=frames_per_batch,\n",
    "    total_frames=total_frames,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "replay_buffer = ReplayBuffer(\n",
    "    storage=LazyTensorStorage(max_size=frames_per_batch, device=device),\n",
    "    sampler=SamplerWithoutReplacement(),\n",
    "    batch_size=torch.Size([frames_per_batch]),\n",
    ")\n",
    "\n",
    "advantage_module = GAE(\n",
    "    gamma=gamma, lmbda=lmbda, value_network=value, average_gae=True, device=device,\n",
    ")\n",
    "\n",
    "loss_module = ClipPPOLoss(\n",
    "    actor_network=actor,\n",
    "    critic_network=value,\n",
    "    clip_epsilon=clip_epsilon,\n",
    "    entropy_bonus=bool(entropy_eps),\n",
    "    entropy_coef=entropy_eps,\n",
    "    critic_coef=1.0,\n",
    "    loss_critic_type=\"smooth_l1\",\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "optim = torch.optim.Adam(loss_module.parameters(), lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optim, total_frames // frames_per_batch, 0.0\n",
    ")\n",
    "\n",
    "collector.rollout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375cea7d",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f654737",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 128/50000 [00:33<3:39:06,  3.79it/s]"
     ]
    }
   ],
   "source": [
    "logs = defaultdict(list)\n",
    "pbar = tqdm(total=total_frames)\n",
    "eval_str = \"\"\n",
    "\n",
    "for i, tensordict_data in enumerate(collector):\n",
    "\n",
    "    tensordict_data = tensordict_data.squeeze(1) # Remove extra added dimension\n",
    "\n",
    "    # we now have a batch of data to work with. Let's learn something from it.\n",
    "    for _ in range(num_epochs):\n",
    "        # We'll need an \"advantage\" signal to make PPO work.\n",
    "        # We re-compute it at each epoch as its value depends on the value\n",
    "        # network which is updated in the inner loop.\n",
    "        advantage_module(tensordict_data)\n",
    "\n",
    "        flat_bs = tensordict_data.batch_size[0]                                 # e.g. 64\n",
    "\n",
    "        # 2) squeeze away any extra dims on obs first\n",
    "        #    (you said your obs was already [batch, C, H, W] here)\n",
    "        obs = tensordict_data[\"observation\"]             # [T, N, C, H, W]\n",
    "        obs = obs.reshape(flat_bs, *obs.shape[1:])       # [flat_bs, C, H, W]\n",
    "\n",
    "        # 3) grab event-shapes for the others\n",
    "        action_shape    = tensordict_data[\"action\"].shape[1:]          # e.g. [6]\n",
    "        logprob_shape   = tensordict_data[\"sample_log_prob\"].shape[1:] # e.g. [6]\n",
    "        # state_value, value_target, advantage are scalar => shape []\n",
    "\n",
    "        # 4) rebuild\n",
    "        flat_td = TensorDict(\n",
    "            {\n",
    "            \"observation\":    obs,\n",
    "            \"action\":         tensordict_data[\"action\"]\n",
    "                                    .reshape(flat_bs, *action_shape),\n",
    "            \"sample_log_prob\": tensordict_data[\"sample_log_prob\"]\n",
    "                                    .reshape(flat_bs, *logprob_shape),\n",
    "            \"state_value\":    tensordict_data[\"state_value\"]\n",
    "                                    .reshape(flat_bs, 1),\n",
    "            \"value_target\":   tensordict_data[\"value_target\"]\n",
    "                                    .reshape(flat_bs, 1),\n",
    "            \"advantage\":      tensordict_data[\"advantage\"]\n",
    "                                    .reshape(flat_bs),      # scalar event\n",
    "            },\n",
    "            batch_size=torch.Size([flat_bs]),\n",
    "            device=tensordict_data.device,\n",
    "        )\n",
    "        replay_buffer.extend(flat_td.cpu())\n",
    "\n",
    "        for _ in range(frames_per_batch // sub_batch_size):\n",
    "            subdata = replay_buffer.sample(sub_batch_size).to(device)\n",
    "            loss_vals = loss_module(subdata)\n",
    "            loss_value = (\n",
    "                loss_vals[\"loss_objective\"]\n",
    "                + loss_vals[\"loss_critic\"]\n",
    "                + loss_vals[\"loss_entropy\"]\n",
    "            )\n",
    "\n",
    "            # Optimization: backward, grad clipping and optimization step\n",
    "            loss_value.backward()\n",
    "            # this is not strictly mandatory but it's good practice to keep\n",
    "            # your gradient norm bounded\n",
    "            torch.nn.utils.clip_grad_norm_(loss_module.parameters(), max_grad_norm)\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "    logs[\"reward\"].append(tensordict_data[\"next\", \"reward\"].mean().item())\n",
    "    pbar.update(tensordict_data.numel())\n",
    "    cum_reward_str = (\n",
    "        f\"average reward={logs['reward'][-1]: 4.4f} (init={logs['reward'][0]: 4.4f})\"\n",
    "    )\n",
    "    logs[\"step_count\"].append(tensordict_data[\"step_count\"].max().item())\n",
    "    stepcount_str = f\"step count (max): {logs['step_count'][-1]}\"\n",
    "    logs[\"lr\"].append(optim.param_groups[0][\"lr\"])\n",
    "    lr_str = f\"lr policy: {logs['lr'][-1]: 4.4f}\"\n",
    "    if i % 10 == 0:\n",
    "        # We evaluate the policy once every 10 batches of data.\n",
    "        # Evaluation is rather simple: execute the policy without exploration\n",
    "        # (take the expected value of the action distribution) for a given\n",
    "        # number of steps (1000, which is our ``env`` horizon).\n",
    "        # The ``rollout`` method of the ``env`` can take a policy as argument:\n",
    "        # it will then execute this policy at each step.\n",
    "        with set_exploration_type(ExplorationType.DETERMINISTIC), torch.no_grad():\n",
    "            # execute a rollout with the trained policy\n",
    "            eval_rollout = env.rollout(1000, actor)\n",
    "            logs[\"eval reward\"].append(eval_rollout[\"next\", \"reward\"].mean().item())\n",
    "            logs[\"eval reward (sum)\"].append(\n",
    "                eval_rollout[\"next\", \"reward\"].sum().item()\n",
    "            )\n",
    "            logs[\"eval step_count\"].append(eval_rollout[\"step_count\"].max().item())\n",
    "            eval_str = (\n",
    "                f\"eval cumulative reward: {logs['eval reward (sum)'][-1]: 4.4f} \"\n",
    "                f\"(init: {logs['eval reward (sum)'][0]: 4.4f}), \"\n",
    "                f\"eval step-count: {logs['eval step_count'][-1]}\"\n",
    "            )\n",
    "            del eval_rollout\n",
    "    pbar.set_description(\", \".join([eval_str, cum_reward_str, stepcount_str, lr_str]))\n",
    "\n",
    "    # We're also using a learning rate scheduler. Like the gradient clipping,\n",
    "    # this is a nice-to-have but nothing necessary for PPO to work.\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aca7b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(logs[\"reward\"])\n",
    "plt.title(\"training rewards (average)\")\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(logs[\"step_count\"])\n",
    "plt.title(\"Max step count (training)\")\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(logs[\"eval reward (sum)\"])\n",
    "plt.title(\"Return (test)\")\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(logs[\"eval step_count\"])\n",
    "plt.title(\"Max step count (test)\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
