{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0b813e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n",
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36960300",
   "metadata": {},
   "source": [
    "## PPO Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dc4177d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from torch import multiprocessing\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# Information\n",
    "from IPython.display import clear_output, display\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "from tensordict.nn import TensorDictModule, TensorDictSequential\n",
    "from torch import nn\n",
    "from torch.distributions import Bernoulli, OneHotCategorical\n",
    "from tensordict import TensorDict\n",
    "\n",
    "# TorchRL\n",
    "from torchrl.collectors import SyncDataCollector, MultiSyncDataCollector\n",
    "from torchrl.data.replay_buffers import ReplayBuffer\n",
    "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
    "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
    "from torchrl.envs import TransformedEnv, Compose, StepCounter \n",
    "from torchrl.envs.transforms import RemoveEmptySpecs\n",
    "from torchrl.envs.utils import check_env_specs, ExplorationType, set_exploration_type\n",
    "from torchrl.modules import ProbabilisticActor, ValueOperator\n",
    "from torchrl.objectives import ClipPPOLoss\n",
    "from torchrl.objectives.value import GAE\n",
    "\n",
    "# Environment\n",
    "from Environment.MazeEnv import TorchRLMazeEnv\n",
    "\n",
    "# Other\n",
    "import uuid\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd75e3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_fork = multiprocessing.get_start_method() == \"fork\"\n",
    "device = (\n",
    "    torch.device(0)\n",
    "    if torch.cuda.is_available() and not is_fork\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "# Model hyper-parameters\n",
    "lr = 3e-3\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "# Environment hyper-parameters\n",
    "env_width = 128\n",
    "env_height = 72\n",
    "batches = 1 # Number of batches\n",
    "size = 1 # Size of each batch\n",
    "\n",
    "# Collector hyper-parameters\n",
    "max_frames = 256\n",
    "frames_per_batch = batches * size * max_frames # number of frames collected per batch\n",
    "total_frames = frames_per_batch * 128  # total number of frames to collect\n",
    "\n",
    "# PPO hyper-parameters\n",
    "sub_batch_size = 64  # cardinality of the sub-samples gathered from the current data in the inner loop\n",
    "num_epochs = 16 # optimization steps per batch of data collected\n",
    "clip_epsilon = ( 0.2 )  # clip value for PPO loss\n",
    "gamma = 0.99\n",
    "lmbda = 0.95\n",
    "entropy_eps = 1e-4\n",
    "\n",
    "# Checkpoint saving parameters\n",
    "checkpoint_interval = 8\n",
    "filename = \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af235e9",
   "metadata": {},
   "source": [
    "### Creating the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b586b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TorchRLMazeEnv(width=env_width, height=env_height, batches=batches, size=size, device=device, show_windows=True)\n",
    "\n",
    "#check_env_specs(env)\n",
    "\n",
    "#env.rollout(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b01750",
   "metadata": {},
   "source": [
    "### Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1b51dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MazeCNN(nn.Module):\n",
    "    def __init__(self, num_actions):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=8, stride=4, device=device),  # [3,72,128] -> [32,17,31]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, device=device), # [32,17,31] -> [64,7,14]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, device=device), # [64,7,14] -> [64,5,12]\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(start_dim=1),\n",
    "        )\n",
    "\n",
    "        # Calculate flattened size\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, 3, 72, 128, device=device)\n",
    "            n_flat = self.features(dummy).shape[1]\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(n_flat, 512, device=device),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_actions, device=device),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return self.head(x)\n",
    "\n",
    "# Transforms observations to logits using module\n",
    "actor = TensorDictModule(\n",
    "    module=MazeCNN(6),\n",
    "    in_keys=[\"observation\"],\n",
    "    out_keys=[\"logits\"],\n",
    ")\n",
    "\n",
    "# \n",
    "actor = ProbabilisticActor(\n",
    "    module=actor,\n",
    "    spec=env.action_spec,\n",
    "    in_keys=[\"logits\"],\n",
    "    distribution_class=OneHotCategorical,\n",
    "    return_log_prob=True,\n",
    "    out_keys=[\"action\"],\n",
    ")\n",
    "\n",
    "value = ValueOperator(\n",
    "    module=MazeCNN(1),\n",
    "    in_keys=[\"observation\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f436a862",
   "metadata": {},
   "outputs": [],
   "source": [
    "collector = SyncDataCollector(\n",
    "    env,\n",
    "    actor,\n",
    "    frames_per_batch=frames_per_batch,\n",
    "    total_frames=total_frames,\n",
    "    max_frames_per_traj=max_frames,\n",
    "    reset_at_each_iter=True,\n",
    "    split_trajs=False,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "replay_buffer = ReplayBuffer(\n",
    "    storage=LazyTensorStorage(max_size=frames_per_batch, device=device),\n",
    "    sampler=SamplerWithoutReplacement(),\n",
    "    batch_size=torch.Size([batches * size * frames_per_batch]),\n",
    ")\n",
    "\n",
    "advantage_module = GAE(\n",
    "    gamma=gamma, lmbda=lmbda, value_network=value, average_gae=True, device=device,\n",
    ")\n",
    "\n",
    "loss_module = ClipPPOLoss(\n",
    "    actor_network=actor,\n",
    "    critic_network=value,\n",
    "    clip_epsilon=clip_epsilon,\n",
    "    entropy_bonus=bool(entropy_eps),\n",
    "    entropy_coef=entropy_eps,\n",
    "    critic_coef=1.0,\n",
    "    loss_critic_type=\"smooth_l1\",\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "optim = torch.optim.Adam(loss_module.parameters(), lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optim, total_frames // frames_per_batch, 1e-4\n",
    ")\n",
    "\n",
    "logs = defaultdict(list)\n",
    "\n",
    "#collector.rollout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375cea7d",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f654737",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = tqdm(total=total_frames)\n",
    "pbar.set_description(\"Training \")\n",
    "\n",
    "# Save model\n",
    "lastrun = int((list(os.listdir(\"checkpoints\"))[-1]).split(\"_\")[0])\n",
    "runDir = f'./checkpoints/{lastrun + 1}'\n",
    "os.mkdir(runDir)\n",
    "\n",
    "eval_env = TorchRLMazeEnv(width=env_width, height=env_height, batches=1, size=1, device=device, show_windows=True)\n",
    "\n",
    "# We get a batch of data from the collector\n",
    "for i, tensordict_data in enumerate(collector):\n",
    "\n",
    "    tensordict_data = tensordict_data.reshape(-1, *tensordict_data.shape[2:])     # [batches, frames, 3, 72, 128] -> [batches*frames, 3, 72, 128]\n",
    "\n",
    "    for _ in range(num_epochs):\n",
    "        advantage_module(tensordict_data)\n",
    "\n",
    "        # Get the tensor to the correct shape\n",
    "\n",
    "        flat_bs = tensordict_data.batch_size[0]\n",
    "        obs = tensordict_data[\"observation\"]\n",
    "        obs = obs.reshape(flat_bs, *obs.shape[1:])\n",
    "        action_shape    = tensordict_data[\"action\"].shape[1:]\n",
    "        logprob_shape   = tensordict_data[\"sample_log_prob\"].shape[1:]\n",
    "        flat_td = TensorDict(\n",
    "            {\n",
    "            \"observation\":    obs,\n",
    "            \"action\":         tensordict_data[\"action\"]\n",
    "                                    .reshape(flat_bs, *action_shape),\n",
    "            \"sample_log_prob\": tensordict_data[\"sample_log_prob\"]\n",
    "                                    .reshape(flat_bs, *logprob_shape),\n",
    "            \"state_value\":    tensordict_data[\"state_value\"]\n",
    "                                    .reshape(flat_bs, 1),\n",
    "            \"value_target\":   tensordict_data[\"value_target\"]\n",
    "                                    .reshape(flat_bs, 1),\n",
    "            \"advantage\":      tensordict_data[\"advantage\"]\n",
    "                                    .reshape(flat_bs),\n",
    "            },\n",
    "            batch_size=torch.Size([flat_bs]),\n",
    "            device=tensordict_data.device,\n",
    "        )\n",
    "\n",
    "        # Add the data to the replay buffer\n",
    "        replay_buffer.extend(flat_td.cpu())\n",
    "\n",
    "        # Sample sub-batches from the replay buffer\n",
    "        for _ in range(frames_per_batch // sub_batch_size):\n",
    "            subdata = replay_buffer.sample(sub_batch_size)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss_vals = loss_module(subdata.to(device))\n",
    "            loss_value = (\n",
    "                loss_vals[\"loss_objective\"]\n",
    "                + loss_vals[\"loss_critic\"]\n",
    "                + loss_vals[\"loss_entropy\"]\n",
    "            )\n",
    "            \n",
    "            # Backpropagate and optimize\n",
    "            loss_value.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(loss_module.parameters(), max_grad_norm)\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "    logs[\"train_reward\"].append( tensordict_data[\"next\", \"reward\"].mean().item() )\n",
    "    #logs[\"train_steps_min\"].append( tensordict_data[\"step_count\"].min().item() )\n",
    "    #logs[\"train_steps_max\"].append( tensordict_data[\"step_count\"].max().item() )\n",
    "\n",
    "    # Execute the policy without exploration\n",
    "    if i % checkpoint_interval == 0:\n",
    "        pbar.set_description(\"Evaluation \")\n",
    "        with set_exploration_type(ExplorationType.DETERMINISTIC), torch.no_grad():\n",
    "            # Run the env with the actors value\n",
    "            eval_rollout = eval_env.rollout(256, actor)\n",
    "\n",
    "            # Save the evaluation data\n",
    "            logs[\"eval_reward\"].append( eval_rollout[\"next\", \"reward\"].mean().item() )\n",
    "            #logs[\"eval_steps\"].append( eval_rollout[\"step_count\"].min().item() )\n",
    "            del eval_rollout\n",
    "\n",
    "            # Save a checkpoint\n",
    "            filename = f'{runDir}/{i}.ch'\n",
    "            checkpoint = {\n",
    "                'model_state_dict': actor.module.state_dict(),\n",
    "            }\n",
    "\n",
    "            torch.save(checkpoint, filename)\n",
    "\n",
    "    pbar.set_description(\"Training \")\n",
    "\n",
    "    # Update plot data\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    # Rebuild the figure from scratch\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(6, 3))\n",
    "    axes = axes.flatten()\n",
    "    titles = [\n",
    "        \"Avg Reward (Train)\", \"Avg Reward (Eval)\",\n",
    "        #\"Min Steps (Train)\", \"Max Steps (Train)\",\n",
    "    ]\n",
    "    data = [\n",
    "        (logs[\"train_reward\"],   \"blue\"),\n",
    "        (logs[\"eval_reward\"],    \"green\"),\n",
    "        #(logs[\"train_steps_min\"],    \"red\"),\n",
    "        #(logs[\"train_steps_max\"],     \"orange\"),\n",
    "    ]\n",
    "\n",
    "    for ax, title, (y, color) in zip(axes, titles, data):\n",
    "        ax.plot(y, color=color)\n",
    "        ax.set_title(title)\n",
    "        ax.relim()\n",
    "        ax.autoscale_view()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Display the new figure\n",
    "    display(fig)\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Learning rate scheduling\n",
    "    #scheduler.step()\n",
    "    \n",
    "    # Update the progress bar\n",
    "    pbar.update(tensordict_data.numel())\n",
    "\n",
    "# Final save of the model\n",
    "filename = f'{runDir}/_final.ch'\n",
    "checkpoint = {\n",
    "    'model_state_dict': actor.module.state_dict(),\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23840d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model for testing\n",
    "if filename == \"\":\n",
    "    # Load the last checkpoint of the last run\n",
    "    anyID = list(os.listdir(\"checkpoints\"))[-1]\n",
    "    filename = \"checkpoints/{}/_final.ch\".format(anyID)\n",
    "    runDir = f'./checkpoints/{anyID}'\n",
    "else:\n",
    "    with open(f'{runDir}/logs.json', 'w') as f:\n",
    "        f.write(json.dumps(logs))\n",
    "\n",
    "l_checkpoint = torch.load(filename)\n",
    "\n",
    "actor.module.state_dict(l_checkpoint[\"model_state_dict\"])\n",
    "\n",
    "env = TorchRLMazeEnv(width=env_width, height=env_height, batches=1, size=1, device=device, show_windows=True)\n",
    "env = TransformedEnv(env, Compose( RemoveEmptySpecs(), StepCounter() ))\n",
    "\n",
    "env.rollout(512, actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008b11e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{runDir}/logs.json', 'r') as f:\n",
    "    logs = json.load(f)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(6, 3))\n",
    "axes = axes.flatten()\n",
    "titles = [\n",
    "    \"Avg Reward (Train)\", \"Avg Reward (Eval)\",\n",
    "    #\"Min Steps (Train)\", \"Max Steps (Train)\",\n",
    "]\n",
    "data = [\n",
    "    (logs[\"train_reward\"],   \"blue\"),\n",
    "    (logs[\"eval_reward\"],    \"green\"),\n",
    "    #(logs[\"train_steps_min\"],    \"red\"),\n",
    "    #(logs[\"train_steps_max\"],     \"orange\"),\n",
    "]\n",
    "\n",
    "for ax, title, (y, color) in zip(axes, titles, data):\n",
    "    ax.plot(y, color=color)\n",
    "    ax.set_title(title)\n",
    "    ax.relim()\n",
    "    ax.autoscale_view()\n",
    "\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
