{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0b813e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n",
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36960300",
   "metadata": {},
   "source": [
    "## PPO Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dc4177d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from torch import multiprocessing\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# Information\n",
    "from IPython.display import clear_output, display\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "from tensordict.nn import TensorDictModule, TensorDictSequential\n",
    "from torch import nn\n",
    "from torch.distributions import Bernoulli, OneHotCategorical\n",
    "from tensordict import TensorDict\n",
    "\n",
    "# TorchRL\n",
    "from torchrl.collectors import SyncDataCollector, MultiSyncDataCollector\n",
    "from torchrl.data.replay_buffers import ReplayBuffer\n",
    "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
    "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
    "from torchrl.envs import TransformedEnv, Compose, StepCounter \n",
    "from torchrl.envs.transforms import RemoveEmptySpecs\n",
    "from torchrl.envs.utils import check_env_specs, ExplorationType, set_exploration_type\n",
    "from torchrl.modules import ProbabilisticActor, ValueOperator\n",
    "from torchrl.objectives import ClipPPOLoss\n",
    "from torchrl.objectives.value import GAE\n",
    "\n",
    "# Environment\n",
    "from Environment.MazeEnv import TorchRLMazeEnv\n",
    "\n",
    "# Other\n",
    "import uuid\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd75e3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_fork = multiprocessing.get_start_method() == \"fork\"\n",
    "device = (\n",
    "    torch.device(0)\n",
    "    if torch.cuda.is_available() and not is_fork\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "# Model hyper-parameters\n",
    "lr = 3e-3\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "# Environment hyper-parameters\n",
    "env_width = 128\n",
    "env_height = 72\n",
    "batches = 1 # Number of batches\n",
    "size = 1 # Size of each batch\n",
    "\n",
    "# Collector hyper-parameters\n",
    "max_frames = 128\n",
    "frames_per_batch = batches * size * max_frames # number of frames collected per batch\n",
    "total_frames = frames_per_batch * 2048000  # total number of frames to collect\n",
    "\n",
    "# PPO hyper-parameters\n",
    "sub_batch_size = max_frames / 2  # cardinality of the sub-samples gathered from the current data in the inner loop\n",
    "num_epochs = 16 # optimization steps per batch of data collected\n",
    "clip_epsilon = ( 0.2 )  # clip value for PPO loss\n",
    "gamma = 0.99\n",
    "lmbda = 0.95\n",
    "entropy_eps = 1e-4\n",
    "\n",
    "# Checkpoint saving parameters\n",
    "checkpoint_interval = 8\n",
    "filename = \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af235e9",
   "metadata": {},
   "source": [
    "### Creating the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b586b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TorchRLMazeEnv(width=env_width, height=env_height, batches=batches, size=size, device=device, show_windows=True)\n",
    "\n",
    "#check_env_specs(env)\n",
    "\n",
    "#env.rollout(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b01750",
   "metadata": {},
   "source": [
    "### Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1b51dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MazeCNN(nn.Module):\n",
    "    def __init__(self, num_actions):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=2, stride=1, device=device),  # [3,72,128] -> [32,17,31]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=2, stride=2, device=device), # [32,17,31] -> [64,7,14]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=2, stride=2, device=device), # [32,17,31] -> [64,7,14]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=2, stride=2, device=device), # [32,17,31] -> [64,7,14]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=2, stride=1, device=device), # [64,7,14] -> [64,5,12]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=2, stride=1, device=device), # [64,7,14] -> [64,5,12]\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(start_dim=1),\n",
    "        )\n",
    "\n",
    "        # Calculate flattened size\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, 3, 72, 128, device=device)\n",
    "            n_flat = self.features(dummy).shape[1]\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(n_flat, 512, device=device),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_actions, device=device),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return self.head(x)\n",
    "\n",
    "# Transforms observations to logits using module\n",
    "actor = TensorDictModule(\n",
    "    module=MazeCNN(6),\n",
    "    in_keys=[\"observation\"],\n",
    "    out_keys=[\"logits\"],\n",
    ")\n",
    "\n",
    "# \n",
    "actor = ProbabilisticActor(\n",
    "    module=actor,\n",
    "    spec=env.action_spec,\n",
    "    in_keys=[\"logits\"],\n",
    "    distribution_class=OneHotCategorical,\n",
    "    return_log_prob=True,\n",
    "    out_keys=[\"action\"],\n",
    ")\n",
    "\n",
    "value = ValueOperator(\n",
    "    module=MazeCNN(1),\n",
    "    in_keys=[\"observation\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f436a862",
   "metadata": {},
   "outputs": [],
   "source": [
    "collector = SyncDataCollector(\n",
    "    env,\n",
    "    actor,\n",
    "    frames_per_batch=frames_per_batch,\n",
    "    total_frames=total_frames,\n",
    "    max_frames_per_traj=max_frames,\n",
    "    reset_at_each_iter=True,\n",
    "    split_trajs=False,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# replay_buffer = ReplayBuffer(\n",
    "#     storage=LazyTensorStorage(max_size=frames_per_batch, device=device),\n",
    "#     sampler=SamplerWithoutReplacement(),\n",
    "#     batch_size=torch.Size([batches * size * frames_per_batch]),\n",
    "# )\n",
    "\n",
    "replay_buffer = []\n",
    "\n",
    "advantage_module = GAE(\n",
    "    gamma=gamma, lmbda=lmbda, value_network=value, average_gae=True, device=device,\n",
    ")\n",
    "\n",
    "loss_module = ClipPPOLoss(\n",
    "    actor_network=actor,\n",
    "    critic_network=value,\n",
    "    clip_epsilon=clip_epsilon,\n",
    "    entropy_bonus=bool(entropy_eps),\n",
    "    entropy_coef=entropy_eps,\n",
    "    critic_coef=1.0,\n",
    "    loss_critic_type=\"smooth_l1\",\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "optim = torch.optim.Adam(loss_module.parameters(), lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optim, total_frames // frames_per_batch, 1e-4\n",
    ")\n",
    "\n",
    "logs = defaultdict(list)\n",
    "\n",
    "#collector.rollout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375cea7d",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f654737",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training :   0%|          | 0/262144000 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pygsampleslet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m eval_env = TorchRLMazeEnv(width=env_width, height=env_height, batches=\u001b[32m1\u001b[39m, size=\u001b[32m1\u001b[39m, device=device, show_windows=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# We get a batch of data from the collector\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensordict_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcollector\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mReplayBuffer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLazyTensorStorage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframes_per_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[43msampler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSamplerWithoutReplacement\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatches\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_per_batch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensordict_data\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensordict_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mtensordict_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# [batches, frames, 3, 72, 128] -> [batches*frames, 3, 72, 128]\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/choice-playgrounds/.venv/lib/python3.12/site-packages/torchrl/collectors/collectors.py:327\u001b[39m, in \u001b[36mDataCollectorBase.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Iterator[TensorDictBase]:\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iterator()\n\u001b[32m    328\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    329\u001b[39m         \u001b[38;5;28mself\u001b[39m.shutdown()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/choice-playgrounds/.venv/lib/python3.12/site-packages/torchrl/collectors/collectors.py:1206\u001b[39m, in \u001b[36mSyncDataCollector.iterator\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1204\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._frames < \u001b[38;5;28mself\u001b[39m.total_frames:\n\u001b[32m   1205\u001b[39m     \u001b[38;5;28mself\u001b[39m._iter += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1206\u001b[39m     tensordict_out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrollout\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1207\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tensordict_out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1208\u001b[39m         \u001b[38;5;66;03m# if a replay buffer is passed and self.extend_buffer=False, there is no tensordict_out\u001b[39;00m\n\u001b[32m   1209\u001b[39m         \u001b[38;5;66;03m#  frames are updated within the rollout function\u001b[39;00m\n\u001b[32m   1210\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/choice-playgrounds/.venv/lib/python3.12/site-packages/torchrl/_utils.py:601\u001b[39m, in \u001b[36maccept_remote_rref_invocation.<locals>.unpack_rref_and_invoke_function\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    599\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _os_is_windows \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, torch._C._distributed_rpc.PyRRef):\n\u001b[32m    600\u001b[39m     \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28mself\u001b[39m.local_value()\n\u001b[32m--> \u001b[39m\u001b[32m601\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/choice-playgrounds/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/choice-playgrounds/.venv/lib/python3.12/site-packages/torchrl/collectors/collectors.py:1458\u001b[39m, in \u001b[36mSyncDataCollector.rollout\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1456\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1457\u001b[39m     env_input = \u001b[38;5;28mself\u001b[39m._shuttle\n\u001b[32m-> \u001b[39m\u001b[32m1458\u001b[39m env_output, env_next_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep_and_maybe_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1460\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shuttle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m env_output:\n\u001b[32m   1461\u001b[39m     \u001b[38;5;66;03m# ad-hoc update shuttle\u001b[39;00m\n\u001b[32m   1462\u001b[39m     next_data = env_output.get(\u001b[33m\"\u001b[39m\u001b[33mnext\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/choice-playgrounds/.venv/lib/python3.12/site-packages/torchrl/envs/common.py:3585\u001b[39m, in \u001b[36mEnvBase.step_and_maybe_reset\u001b[39m\u001b[34m(self, tensordict)\u001b[39m\n\u001b[32m   3583\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tensordict.device != \u001b[38;5;28mself\u001b[39m.device:\n\u001b[32m   3584\u001b[39m     tensordict = tensordict.to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m-> \u001b[39m\u001b[32m3585\u001b[39m tensordict = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3586\u001b[39m \u001b[38;5;66;03m# done and truncated are in done_keys\u001b[39;00m\n\u001b[32m   3587\u001b[39m \u001b[38;5;66;03m# We read if any key is done.\u001b[39;00m\n\u001b[32m   3588\u001b[39m tensordict_ = \u001b[38;5;28mself\u001b[39m._step_mdp(tensordict)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/choice-playgrounds/.venv/lib/python3.12/site-packages/torchrl/envs/common.py:2031\u001b[39m, in \u001b[36mEnvBase.step\u001b[39m\u001b[34m(self, tensordict)\u001b[39m\n\u001b[32m   2028\u001b[39m next_preset = tensordict.get(\u001b[33m\"\u001b[39m\u001b[33mnext\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m   2030\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m next_tensordict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2031\u001b[39m     next_tensordict = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2032\u001b[39m     next_tensordict = \u001b[38;5;28mself\u001b[39m._step_proc_data(next_tensordict)\n\u001b[32m   2033\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m next_preset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2034\u001b[39m     \u001b[38;5;66;03m# tensordict could already have a \"next\" key\u001b[39;00m\n\u001b[32m   2035\u001b[39m     \u001b[38;5;66;03m# this could be done more efficiently by not excluding but just passing\u001b[39;00m\n\u001b[32m   2036\u001b[39m     \u001b[38;5;66;03m# the necessary keys\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/choice-playgrounds/.venv/lib/python3.12/site-packages/torchrl/envs/transforms/transforms.py:1129\u001b[39m, in \u001b[36mTransformedEnv._step\u001b[39m\u001b[34m(self, tensordict)\u001b[39m\n\u001b[32m   1126\u001b[39m         tensordict_batch_size = \u001b[38;5;28mself\u001b[39m.batch_size\n\u001b[32m   1128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m next_tensordict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1129\u001b[39m     next_tensordict = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_env\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict_in\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1130\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m next_preset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1131\u001b[39m         \u001b[38;5;66;03m# tensordict could already have a \"next\" key\u001b[39;00m\n\u001b[32m   1132\u001b[39m         \u001b[38;5;66;03m# this could be done more efficiently by not excluding but just passing\u001b[39;00m\n\u001b[32m   1133\u001b[39m         \u001b[38;5;66;03m# the necessary keys\u001b[39;00m\n\u001b[32m   1134\u001b[39m         next_tensordict.update(\n\u001b[32m   1135\u001b[39m             next_preset.exclude(*next_tensordict.keys(\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[32m   1136\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/choice-playgrounds/Environment/MazeEnv.py:769\u001b[39m, in \u001b[36mTorchRLMazeEnv._step\u001b[39m\u001b[34m(self, tensordict)\u001b[39m\n\u001b[32m    766\u001b[39m \u001b[38;5;66;03m# Convert to native actions\u001b[39;00m\n\u001b[32m    767\u001b[39m native_actions = actions.cpu().numpy()\n\u001b[32m--> \u001b[39m\u001b[32m769\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnative_actions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_count\u001b[49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# a list of N = batches*size tuples\u001b[39;00m\n\u001b[32m    770\u001b[39m imgs, rewards, dones = \u001b[38;5;28mzip\u001b[39m(*results)      \u001b[38;5;66;03m# each is a length-N tuple\u001b[39;00m\n\u001b[32m    772\u001b[39m obs    = torch.stack([torch.tensor(img)   \u001b[38;5;28;01mfor\u001b[39;00m img   \u001b[38;5;129;01min\u001b[39;00m imgs],    dim=\u001b[32m0\u001b[39m).to(\u001b[38;5;28mself\u001b[39m.device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/choice-playgrounds/Environment/MazeEnv.py:111\u001b[39m, in \u001b[36mmultiGames.step\u001b[39m\u001b[34m(self, inputs, stepCount)\u001b[39m\n\u001b[32m    108\u001b[39m results = [\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.batches)]\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.batches):\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     \u001b[43mthreadWorker\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgames\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstepCount\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[38;5;66;03m# for i in range(self.batches):\u001b[39;00m\n\u001b[32m    120\u001b[39m \u001b[38;5;66;03m#     t = threading.Thread(\u001b[39;00m\n\u001b[32m    121\u001b[39m \u001b[38;5;66;03m#         target=threadWorker,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    134\u001b[39m \n\u001b[32m    135\u001b[39m \u001b[38;5;66;03m# flatten\u001b[39;00m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [item \u001b[38;5;28;01mfor\u001b[39;00m sublist \u001b[38;5;129;01min\u001b[39;00m results \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m sublist]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/choice-playgrounds/Environment/MazeEnv.py:80\u001b[39m, in \u001b[36mthreadWorker\u001b[39m\u001b[34m(wrappers, actions, results, index, stepCount)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mthreadWorker\u001b[39m(wrappers: \u001b[38;5;28mlist\u001b[39m[\u001b[33m'\u001b[39m\u001b[33mGameWrapper\u001b[39m\u001b[33m'\u001b[39m], actions: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mbool\u001b[39m]], results: \u001b[38;5;28mlist\u001b[39m, index: \u001b[38;5;28mint\u001b[39m, stepCount: \u001b[38;5;28mint\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     results[index] = [\u001b[43mw\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstepCount\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m w, a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(wrappers, actions)]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/choice-playgrounds/Environment/MazeEnv.py:602\u001b[39m, in \u001b[36mGameWrapper.step\u001b[39m\u001b[34m(self, actions, stepCount)\u001b[39m\n\u001b[32m    599\u001b[39m \u001b[38;5;28mself\u001b[39m.game.set_keys = actions\n\u001b[32m    601\u001b[39m prev_pos = np.array([\u001b[38;5;28mself\u001b[39m.game.z])\n\u001b[32m--> \u001b[39m\u001b[32m602\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_scheduler\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    603\u001b[39m new_pos = np.array([\u001b[38;5;28mself\u001b[39m.game.z])\n\u001b[32m    605\u001b[39m \u001b[38;5;66;03m# Get the image of the game\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/choice-playgrounds/Environment/MazeEnv.py:655\u001b[39m, in \u001b[36mGameWrapper._run_scheduler\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_scheduler\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m     \u001b[43mpygsampleslet\u001b[49m.clock.tick()\n\u001b[32m    657\u001b[39m     \u001b[38;5;28mself\u001b[39m.game.switch_to()\n\u001b[32m    658\u001b[39m     \u001b[38;5;28mself\u001b[39m.game.dispatch_events()\n",
      "\u001b[31mNameError\u001b[39m: name 'pygsampleslet' is not defined"
     ]
    }
   ],
   "source": [
    "pbar = tqdm(total=total_frames)\n",
    "pbar.set_description(\"Training \")\n",
    "\n",
    "import uuid\n",
    "\n",
    "# Save model\n",
    "# lastRunDir = list(os.listdir(\"checkpoints\"))\n",
    "# lastRunDir.sort()\n",
    "# lastrun = int(lastRunDir[-1].split(\"_\")[0])\n",
    "runDir = f'./checkpoints/{uuid.uuid4()}'\n",
    "os.mkdir(runDir)\n",
    "\n",
    "eval_env = TorchRLMazeEnv(width=env_width, height=env_height, batches=1, size=1, device=device, show_windows=True)\n",
    "\n",
    "# We get a batch of data from the collector\n",
    "for i, tensordict_data in enumerate(collector):\n",
    "\n",
    "    replay_buffer = ReplayBuffer(\n",
    "        storage=LazyTensorStorage(max_size=frames_per_batch, device=device),\n",
    "        sampler=SamplerWithoutReplacement(),\n",
    "        batch_size=torch.Size([batches * size * frames_per_batch]),\n",
    "    )\n",
    "\n",
    "    tensordict_data = tensordict_data.reshape(-1, *tensordict_data.shape[2:])     # [batches, frames, 3, 72, 128] -> [batches*frames, 3, 72, 128]\n",
    "\n",
    "    for _ in range(num_epochs):\n",
    "        advantage_module(tensordict_data)\n",
    "\n",
    "        # Get the tensor to the correct shape\n",
    "\n",
    "        flat_bs = tensordict_data.batch_size[0]\n",
    "        obs = tensordict_data[\"observation\"]\n",
    "        obs = obs.reshape(flat_bs, *obs.shape[1:])\n",
    "        action_shape    = tensordict_data[\"action\"].shape[1:]\n",
    "        logprob_shape   = tensordict_data[\"sample_log_prob\"].shape[1:]\n",
    "        flat_td = TensorDict(\n",
    "            {\n",
    "            \"observation\":    obs,\n",
    "            \"action\":         tensordict_data[\"action\"]\n",
    "                                    .reshape(flat_bs, *action_shape),\n",
    "            \"sample_log_prob\": tensordict_data[\"sample_log_prob\"]\n",
    "                                    .reshape(flat_bs, *logprob_shape),\n",
    "            \"state_value\":    tensordict_data[\"state_value\"]\n",
    "                                    .reshape(flat_bs, 1),\n",
    "            \"value_target\":   tensordict_data[\"value_target\"]\n",
    "                                    .reshape(flat_bs, 1),\n",
    "            \"advantage\":      tensordict_data[\"advantage\"]\n",
    "                                    .reshape(flat_bs),\n",
    "            },\n",
    "            batch_size=torch.Size([flat_bs]),\n",
    "            device=tensordict_data.device,\n",
    "        )\n",
    "\n",
    "        # Add the data to the replay buffer\n",
    "        replay_buffer.extend(flat_td.cpu())\n",
    "\n",
    "        # Sample sub-batches from the replay buffer\n",
    "        for _ in range(int(frames_per_batch / sub_batch_size)):\n",
    "            subdata = replay_buffer.sample(int(sub_batch_size))\n",
    "\n",
    "            # Compute the loss\n",
    "            loss_vals = loss_module(subdata.to(device))\n",
    "            loss_value = (\n",
    "                loss_vals[\"loss_objective\"]\n",
    "                + loss_vals[\"loss_critic\"]\n",
    "                + loss_vals[\"loss_entropy\"]\n",
    "            )\n",
    "            \n",
    "            # Backpropagate and optimize\n",
    "            loss_value.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(loss_module.parameters(), max_grad_norm)\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "    logs[\"train_reward\"].append( tensordict_data[\"next\", \"reward\"].mean().item() )\n",
    "    #logs[\"train_steps_min\"].append( tensordict_data[\"step_count\"].min().item() )\n",
    "    #logs[\"train_steps_max\"].append( tensordict_data[\"step_count\"].max().item() )\n",
    "\n",
    "    # Execute the policy without exploration\n",
    "    if i % checkpoint_interval == 0:\n",
    "        pbar.set_description(\"Evaluation \")\n",
    "        with (ExplorationType.DETERMINISTIC), torch.no_grad():\n",
    "            # Run the env with the actors value\n",
    "            eval_rollout = eval_env.rollout(256, actor)\n",
    "\n",
    "            # Save the evaluation data\n",
    "            logs[\"eval_reward\"].append( eval_rollout[\"next\", \"reward\"].mean().item() )\n",
    "            #logs[\"eval_steps\"].append( eval_rollout[\"step_count\"].min().item() )\n",
    "            del eval_rollout\n",
    "\n",
    "            # Save a checkpoint\n",
    "            filename = f'{runDir}/{i}.ch'\n",
    "            checkpoint = {\n",
    "                'model_state_dict': actor.module.state_dict(),\n",
    "            }\n",
    "\n",
    "            torch.save(checkpoint, filename)\n",
    "\n",
    "    pbar.set_description(\"Training \")\n",
    "\n",
    "    # Update plot data\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    # Rebuild the figure from scratch\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(6, 3))\n",
    "    axes = axes.flatten()\n",
    "    titles = [\n",
    "        \"Avg Reward (Train)\", \"Avg Reward (Eval)\",\n",
    "        #\"Min Steps (Train)\", \"Max Steps (Train)\",\n",
    "    ]\n",
    "    data = [\n",
    "        (logs[\"train_reward\"],   \"blue\"),\n",
    "        (logs[\"eval_reward\"],    \"green\"),\n",
    "        #(logs[\"train_steps_min\"],    \"red\"),\n",
    "        #(logs[\"train_steps_max\"],     \"orange\"),\n",
    "    ]\n",
    "\n",
    "    for ax, title, (y, color) in zip(axes, titles, data):\n",
    "        ax.plot(y, color=color)\n",
    "        ax.set_title(title)\n",
    "        ax.relim()\n",
    "        ax.autoscale_view()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Display the new figure\n",
    "    display(fig)\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Learning rate scheduling\n",
    "    #scheduler.step()\n",
    "    \n",
    "    # Update the progress bar\n",
    "    pbar.update(tensordict_data.numel())\n",
    "\n",
    "# Final save of the model\n",
    "filename = f'{runDir}/_final.ch'\n",
    "checkpoint = {\n",
    "    'model_state_dict': actor.module.state_dict(),\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23840d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model for testing\n",
    "if filename == \"\":\n",
    "    # Load the last checkpoint of the last run\n",
    "    anyID = list(os.listdir(\"checkpoints\"))[-1]\n",
    "    filename = \"checkpoints/{}/_final.ch\".format(anyID)\n",
    "    runDir = f'./checkpoints/{anyID}'\n",
    "else:\n",
    "    with open(f'{runDir}/logs.json', 'w') as f:\n",
    "        f.write(json.dumps(logs))\n",
    "\n",
    "l_checkpoint = torch.load(filename)\n",
    "\n",
    "actor.module.state_dict(l_checkpoint[\"model_state_dict\"])\n",
    "\n",
    "env = TorchRLMazeEnv(width=env_width, height=env_height, batches=1, size=1, device=device, show_windows=True)\n",
    "env = TransformedEnv(env, Compose( RemoveEmptySpecs(), StepCounter() ))\n",
    "\n",
    "env.rollout(512, actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008b11e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{runDir}/logs.json', 'r') as f:\n",
    "    logs = json.load(f)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(6, 3))\n",
    "axes = axes.flatten()\n",
    "titles = [\n",
    "    \"Avg Reward (Train)\", \"Avg Reward (Eval)\",\n",
    "    #\"Min Steps (Train)\", \"Max Steps (Train)\",\n",
    "]\n",
    "data = [\n",
    "    (logs[\"train_reward\"],   \"blue\"),\n",
    "    (logs[\"eval_reward\"],    \"green\"),\n",
    "    #(logs[\"train_steps_min\"],    \"red\"),\n",
    "    #(logs[\"train_steps_max\"],     \"orange\"),\n",
    "]\n",
    "\n",
    "for ax, title, (y, color) in zip(axes, titles, data):\n",
    "    ax.plot(y, color=color)\n",
    "    ax.set_title(title)\n",
    "    ax.relim()\n",
    "    ax.autoscale_view()\n",
    "\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
